{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from __future__ import division\n",
    "\n",
    "import gym\n",
    "import numpy as np\n",
    "import random\n",
    "import tensorflow as tf\n",
    "import tensorflow.contrib.layers as initializer\n",
    "import scipy.misc\n",
    "import os\n",
    "import sys\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from gridworld_v04 import gameEnv\n",
    "env = gameEnv(partial=False, size=15)\n",
    "#env.renderEnv()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class Agent():\n",
    "    def __init__(self, hiddens = [256, 256, 256], num_frames = 4, state_size = 10,\n",
    "                 action_size = 6, env=None, lr = 1e-3, batch_size = 32, num_episodes = 10000, pre_train_steps = 10000,\n",
    "                 max_ep_length = 500, buffer_size = 50000, start_e = 1, final_e = 0.01, gamma = 0.99,\n",
    "                 tau=0.001, update_freq = 5, load_model = False , logs_dir = \"./logs_dir\"):\n",
    "\n",
    "        self.hiddens = hiddens # Size of the hidden layers\n",
    "        self.num_frames = num_frames # Number of consecutive state frames\n",
    "        self.state_size = state_size # Size of the state vector\n",
    "        self.action_size = action_size  # Number of actions\n",
    "        self.env = env # Save the environmet!\n",
    "        self.lr=lr # learning rate of the optimizer\n",
    "        self.batch_size=batch_size # Size of the experience sample batch\n",
    "        self.num_episodes = num_episodes # Number of game environmet episodes in which we train\n",
    "        self.pre_train_steps = pre_train_steps\n",
    "        self.max_ep_length = max_ep_length # Maximun length (number of actions) of a single train episode\n",
    "        self.buffer_size = buffer_size # Size of the experience replay buffer\n",
    "        self.start_e = start_e # Inital value of the exploration coefficient\n",
    "        self.final_e = final_e # Final value of the exploration coefficient\n",
    "        self.gamma = gamma # Discount factor on the target Q-value\n",
    "        self.tau = tau # Porcentage that determines how much are parameters of mainQN net modified by targetQN\n",
    "        self.update_freq = update_freq # Frecuency of updates of the double DQN\n",
    "        self.logs_dir = logs_dir # Path to store logs and checkpoints\n",
    "\n",
    "        tf.reset_default_graph()\n",
    "\n",
    "        # Instantiate the networks\n",
    "        self.mainQN = DQN(\"mainQN\", state_size, action_size, hiddens, num_frames)\n",
    "        self.targetQN = DQN(\"targetQN\", state_size, action_size, hiddens, num_frames)\n",
    "\n",
    "        #init = tf.global_variables_initializer()\n",
    "        trainables = tf.trainable_variables()\n",
    "\n",
    "        self.target_ops = self.set_target_graph_vars(trainables, tau)\n",
    "\n",
    "        # Create a experience replay buffer & score records\n",
    "        self.exp_buffer = ExperienceBuffer()\n",
    "        self.step_record = [] # for learing process\n",
    "        self.reward_record = [] # for learnig process\n",
    "        self.step_record_test = [] # for testing process\n",
    "        self.reward_record_test = [] # for testing process\n",
    "\n",
    "        self.model_saver = ModelSaver(self.logs_dir)\n",
    "\n",
    "    def learn(self):\n",
    "        print(\"Model Learning\")\n",
    "        init = tf.global_variables_initializer()\n",
    "        sess = tf.Session()\n",
    "        e = self.start_e\n",
    "        current_step = 0\n",
    "\n",
    "        #Code for writing to a file?\n",
    "        #logs_file = self.logs_dir + \"/train_logs.txt\"\n",
    "        #with open(logs_file) as fp:\n",
    "\n",
    "        with sess:\n",
    "\n",
    "            sess.run(init)\n",
    "           # if self.load_model == True:\n",
    "           #     print('Loading Model...')\n",
    "           #     model_saver.restore_model(sess)\n",
    "           #     #ckpt = model_saver_restore_model(sess)\n",
    "\n",
    "            # Set the target network to be equal to the primary network\n",
    "            self.update_target_graph(self.target_ops, sess)\n",
    "\n",
    "            # Start the pre train proces\n",
    "            for episode in range(self.num_episodes):\n",
    "\n",
    "                if episode % 100 == 0:\n",
    "                    print(\"\\n=====\" + \"Episode \" + str(episode) + \" starts =====\" )\n",
    "\n",
    "                episode_exp = ExperienceBuffer()\n",
    "\n",
    "                #Reset environment and get first new observation\n",
    "                s = self.env.reset()\n",
    "\n",
    "                d = False # episode's \"done\" signal\n",
    "                episode_reward_sum = 0\n",
    "                episode_step = 0\n",
    "\n",
    "                #The Q-Network\n",
    "                while episode_step < self.max_ep_length: #If the agent take too long to win, end the trial.\n",
    "                    episode_step += 1\n",
    "\n",
    "                    #Choose an action by greedily (with e chance of random action) from the Q-network\n",
    "                    if np.random.rand(1) < e or current_step < self.pre_train_steps:\n",
    "                        a = np.random.randint(0,4)\n",
    "                    else:\n",
    "                        a = sess.run(self.mainQN.predict,feed_dict={self.mainQN.input:[s]})[0]\n",
    "\n",
    "                    s1,r,d = self.env.step(a)\n",
    "                    current_step += 1\n",
    "                    episode_exp.add(np.reshape(np.array([s,a,r,s1,d]),[1,5])) # Save the experience to our episode buffer.\n",
    "\n",
    "                    # Start train process\n",
    "                    if current_step > self.pre_train_steps:\n",
    "\n",
    "                        if e > self.final_e:\n",
    "                            stepDrop = 1/10000\n",
    "                            e -= stepDrop\n",
    "\n",
    "                        if current_step % self.update_freq == 0:\n",
    "\n",
    "                            train_batch = self.exp_buffer.sample(self.batch_size) #Get a random batch of experiences.\n",
    "\n",
    "                            #Perform the Double-DQN update to the target Q-values\n",
    "                            Q1 = sess.run(self.mainQN.predict,\n",
    "                                          feed_dict={self.mainQN.input:np.vstack(train_batch[:,3])})\n",
    "\n",
    "                            Q2 = sess.run(self.targetQN.Qout,\n",
    "                                          feed_dict={self.targetQN.input:np.vstack(train_batch[:,3])})\n",
    "\n",
    "                            end_multiplier = -(train_batch[:,4] - 1)\n",
    "                            doubleQ = Q2[range(self.batch_size),Q1]\n",
    "                            targetQ = train_batch[:,2] + (self.gamma*doubleQ*end_multiplier)\n",
    "\n",
    "                            # Update the network with our target values.\n",
    "                            _ = sess.run(self.mainQN.updateModel,\n",
    "                                         feed_dict={self.mainQN.input:np.vstack(train_batch[:,0]),\n",
    "                                         self.mainQN.targetQ:targetQ,\n",
    "                                         self.mainQN.actions:train_batch[:,1]})\n",
    "\n",
    "                            # Set the target network to be equal to the primary\n",
    "                            self.update_target_graph(self.target_ops, sess)\n",
    "\n",
    "                    episode_reward_sum += r\n",
    "                    s = s1\n",
    "\n",
    "                    if d == True:\n",
    "                        break\n",
    "\n",
    "                self.exp_buffer.add(episode_exp.buffer)\n",
    "                self.step_record.append(episode_step)\n",
    "                self.reward_record.append(episode_reward_sum)\n",
    "\n",
    "                #Periodically save the model.\n",
    "                if episode % 1000 == 0:\n",
    "                    self.model_saver.save_model(sess, episode)\n",
    "                    print(\"Model saved\")\n",
    "\n",
    "                if len(self.reward_record) % 10 == 0:\n",
    "                    partial_reward = np.mean(self.reward_record[-10:])\n",
    "                    log = str(current_step) +\"\\t\"+ str(partial_reward) +\"\\t\"+ str(e) +\"\\n\"\n",
    "                    #f.write(log)\n",
    "                    print \"%d %4.2f %3.2f\" % (current_step, partial_reward, e)\n",
    "\n",
    "\n",
    "            self.model_saver.save_model(sess, self.num_episodes)\n",
    "        final_log = \"Percent of sucessful episodes: \" + str(sum(self.reward_record)/self.num_episodes) + \"%\"\n",
    "        print(final_log)\n",
    "\n",
    "    def test(self):\n",
    "        current_step = 0\n",
    "        load_model = True\n",
    "        num_test_episodes = 5000\n",
    "\n",
    "        with tf.Session() as sess:\n",
    "            if load_model == True:\n",
    "                print('Loading Model...')\n",
    "                self.model_saver.restore_model(sess)\n",
    "\n",
    "            for episode in range(num_test_episodes):\n",
    "                s = self.env.reset_for_testing()\n",
    "                d = False\n",
    "                episode_step = 0\n",
    "                episode_reword_sum = 0\n",
    "\n",
    "                #Q-Network\n",
    "                while episode_step < self.max_ep_length:\n",
    "                    episode_step += 1\n",
    "                    a = sess.run(self.mainQN.predict, feed_dict={self.mainQN.input:[s]})[0]\n",
    "                    s1, r, d = self.env.step_for_testing(a)\n",
    "\n",
    "                    current_step += 1\n",
    "                    episode_reword_sum += r\n",
    "                    s = s1\n",
    "\n",
    "                    if d == True:\n",
    "                        break\n",
    "\n",
    "        self.step_record_test.append(episode_step)\n",
    "        self.reward_record_test.append(episode_reword_sum)\n",
    "\n",
    "        # print(episode_reword_sum)\n",
    "\n",
    "        if len(self.reward_record_test) % 10 == 0:\n",
    "            partial_reward = np.mean(self.reward_record_test[-10:])\n",
    "            log = str(current_step) +\"\\t\"+ str(partial_reward)\n",
    "            #f.write(log)\n",
    "            print \"%d %4.2f\" % (current_step, partial_reward)\n",
    "\n",
    "        final_log = \"Percent of sucessful episodes: \" + str(sum(self.reward_record_test)/num_test_episodes) + \"%\"\n",
    "        print(final_log)\n",
    "\n",
    "    \"\"\" Auxiliary Methods \"\"\"\n",
    "    # Originally called updateTargetGraph\n",
    "    def set_target_graph_vars(self, tfVars, tau):\n",
    "        total_vars = len(tfVars)\n",
    "        op_holder = []\n",
    "\n",
    "        for idx,var in enumerate(tfVars[0:total_vars//2]): # Select the first half of the variables (mainQ net)\n",
    "            op_holder.append( tfVars[idx+total_vars//2].assign((var.value()*tau)+((1-tau)*tfVars[idx+total_vars//2].value())))\n",
    "\n",
    "        return op_holder\n",
    "    # Originally called updateTarget\n",
    "    def update_target_graph(self, op_holder, sess):\n",
    "        for op in op_holder:\n",
    "            sess.run(op)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class DQN():\n",
    "    def __init__(self, net_name, state_size, action_size, hiddens, num_frames):\n",
    "        self.net_name = net_name\n",
    "\n",
    "        with tf.variable_scope(self.net_name):\n",
    "\n",
    "            self.input = tf.placeholder(shape=[None, state_size], dtype=tf.float32)\n",
    "            #self.input_state = tf.reshape(self.state, [-1, num_frames * state_size])\n",
    "\n",
    "            # Weights of each layer\n",
    "            self.W = {\n",
    "                'W1': self.init_weight(\"W1\", [state_size, hiddens[0]]),\n",
    "                'W2': self.init_weight(\"W2\", [hiddens[0], hiddens[1]]),\n",
    "                'W3': self.init_weight(\"W3\", [hiddens[1], hiddens[2]]),\n",
    "                'AW': self.init_weight(\"AW\", [hiddens[2]//2, action_size]),\n",
    "                'VM': self.init_weight(\"VM\", [hiddens[2]//2, 1])\n",
    "            }\n",
    "\n",
    "            self.b = {\n",
    "                'b1': self.init_bias(\"b1\", hiddens[0]),\n",
    "                'b2': self.init_bias(\"b2\", hiddens[1]),\n",
    "                'b3': self.init_bias(\"b3\", hiddens[2])\n",
    "            }\n",
    "\n",
    "            # Layers\n",
    "            self.hidden1 = tf.nn.relu(tf.add(tf.matmul(self.input, self.W['W1']), self.b['b1']))\n",
    "            self.hidden2 = tf.nn.relu(tf.add(tf.matmul(self.hidden1, self.W['W2']), self.b['b2']))\n",
    "            self.hidden3 = tf.nn.relu(tf.add(tf.matmul(self.hidden2, self.W['W3']), self.b['b3']))\n",
    "\n",
    "            # Compute the Advantage, Value, and total Q value\n",
    "            self.A, self.V = tf.split(self.hidden3, 2, 1)\n",
    "            self.Advantage = tf.matmul(self.A, self.W['AW'])\n",
    "            self.Value = tf.matmul(self.V, self.W['VM'])\n",
    "            self.Qout = self.Value + tf.subtract(self.Advantage, tf.reduce_mean(self.Advantage, axis=1, keep_dims=True))\n",
    "\n",
    "            # Calcultate the action with highest Q value\n",
    "            self.predict = tf.argmax(self.Qout, 1)\n",
    "\n",
    "            # Compute the loss (sum of squared differences)\n",
    "            self.targetQ = tf.placeholder(shape=[None], dtype=tf.float32)\n",
    "            self.actions = tf.placeholder(shape=[None], dtype=tf.int32)\n",
    "            self.actions_one_hot = tf.one_hot(self.actions, action_size, dtype=tf.float32)\n",
    "\n",
    "            self.Q = tf.reduce_sum(tf.multiply(self.Qout, self.actions_one_hot), axis=1)\n",
    "            self.td_error = tf.square(self.targetQ - self.Q)\n",
    "            self.loss = tf.reduce_mean(self.td_error)\n",
    "\n",
    "            self.trainer = tf.train.AdamOptimizer(learning_rate=0.0001)\n",
    "            self.updateModel = self.trainer.minimize(self.loss)\n",
    "\n",
    "    def init_weight(self, name, shape):\n",
    "        return tf.get_variable(name=name, shape=shape, initializer=initializer.xavier_initializer(), dtype=tf.float32)\n",
    "\n",
    "    def init_bias(self, name, shape):\n",
    "        return tf.Variable(tf.random_normal([shape]))\n",
    "        #initializer = tf.constant(np.random.rand(shape))\n",
    "        #return tf.get_variable(name=name, initializer=initializer, dtype=tf.float32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class ExperienceBuffer():\n",
    "    def __init__(self, buffer_size = 50000):\n",
    "        self.buffer = []\n",
    "        self.buffer_size = buffer_size\n",
    "\n",
    "    def add(self, experience):\n",
    "\n",
    "        if len(self.buffer) + len(experience) >= self.buffer_size:\n",
    "            self.buffer[0:(len(experience) + len(self.buffer)) - self.buffer_size] = []\n",
    "\n",
    "        self.buffer.extend(experience)\n",
    "\n",
    "    def sample(self, size):\n",
    "        return np.reshape(np.array(random.sample(self.buffer, size)),[size,5])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class ModelSaver():\n",
    "    def __init__(self, path):\n",
    "        self.saver = tf.train.Saver()\n",
    "        self.ckptPath = path\n",
    "        if not os.path.exists(path):\n",
    "            os.makedirs(path)\n",
    "\n",
    "    def restore_model(self, sess):\n",
    "        ckpt = tf.train.get_checkpoint_state(self.ckptPath)\n",
    "        self.saver.restore(sess, ckpt.model_checkpoint_path)\n",
    "\n",
    "    def save_model(self, sess, num_episode):\n",
    "        self.saver.save(sess, self.ckptPath+'/model-'+str(num_episode)+'.cptk')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model Learning\n",
      "\n",
      "=====Episode 0 starts =====\n",
      "Model saved\n",
      "4511 -69.60 1.00\n",
      "9018 -74.90 1.00\n"
     ]
    }
   ],
   "source": [
    "def main():\n",
    "    env = gameEnv(partial = False, size=15)\n",
    "        \n",
    "    # The parameters of Agent class are as follows:\n",
    "    # hiddens, num_frames, state_size, action_size, env, lr, batch_size, num_episodes, pre_train_steps,\n",
    "    # max_ep_length, buffer_size, start_e, final_e, gamma, tau, update_freq, load_model, logs_dir\n",
    "    agent = Agent([256, 256, 256], 1, len(env.mapEnv()), 4, env, 1e-3, 32, 10000, 10000, 500, 50000, 1, 0.01, 0.99, 0.001, 5, False, \"./logs_dir\")\n",
    "    agent.learn()\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    main()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
