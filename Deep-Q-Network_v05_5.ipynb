{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from __future__ import division \n",
    "\n",
    "import gym\n",
    "import numpy as np\n",
    "import random\n",
    "import tensorflow as tf \n",
    "import tensorflow.contrib.slim as slim\n",
    "import tensorflow.contrib.layers as initializer\n",
    "import matplotlib.pyplot as plt\n",
    "import scipy.misc\n",
    "import os\n",
    "import sys\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from gridworld_v04 import gameEnv\n",
    "#env = gameEnv(partial=False, size=15)\n",
    "#env.renderEnv()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class Agent():\n",
    "    def __init__(self, hiddens = [256, 256, 256], num_frames = 4, state_size = 10,\n",
    "                 action_size = 6, env=None, lr = 1e-3, batch_size = 32, num_episodes = 5000, pre_train_steps = 10000,\n",
    "                 max_ep_length = 300, buffer_size = 50000, start_e = 1, final_e = 0.01, gamma = 0.99,\n",
    "                 tau=0.001, update_freq = 5, load_model = False , logs_dir = \"./logs_dir\"):\n",
    "\n",
    "        self.hiddens = hiddens # Size of the hidden layers\n",
    "        self.num_frames = num_frames # Number of consecutive state frames\n",
    "        self.state_size = state_size # Size of the state vector\n",
    "        self.action_size = action_size  # Number of actions\n",
    "        self.env = env # Save the environmet!\n",
    "        self.lr=lr # learning rate of the optimizer\n",
    "        self.batch_size=batch_size # Size of the experience sample batch\n",
    "        self.num_episodes = num_episodes # Number of game environmet episodes in which we train\n",
    "        self.pre_train_steps = pre_train_steps\n",
    "        self.max_ep_length = max_ep_length # Maximun length (number of actions) of a single train episode\n",
    "        self.buffer_size = buffer_size # Size of the experience replay buffer\n",
    "        self.start_e = start_e # Inital value of the exploration coefficient\n",
    "        self.final_e = final_e # Final value of the exploration coefficient\n",
    "        self.gamma = gamma # Discount factor on the target Q-value\n",
    "        self.tau = tau # Porcentage that determines how much are parameters of mainQN net modified by targetQN\n",
    "        self.update_freq = update_freq # Frecuency of updates of the double DQN\n",
    "        self.logs_dir = logs_dir # Path to store logs and checkpoints\n",
    "\n",
    "        tf.reset_default_graph()\n",
    "\n",
    "        # Instantiate the networks\n",
    "        #self.mainQN = DQN(\"mainQN\", state_size, action_size, hiddens, num_frames)\n",
    "        self.mainQN = DQN(\"mainQN\", len(self.env.objects) * 2, self.env.actions, hiddens, num_frames)\n",
    "        self.targetQN = DQN(\"targetQN\", len(self.env.objects) * 2, self.env.actions, hiddens, num_frames)\n",
    "\n",
    "        init = tf.global_variables_initializer()\n",
    "        trainables = tf.trainable_variables()\n",
    "\n",
    "        self.target_ops = self.set_target_graph_vars(trainables, tau)\n",
    "\n",
    "        # Create a experience replay buffer & score records\n",
    "        self.exp_buffer = ExperienceBuffer()\n",
    "        self.step_record = []\n",
    "        self.reward_record = []\n",
    "\n",
    "        self.model_saver = ModelSaver(self.logs_dir)\n",
    "        \n",
    "    def learn(self):\n",
    "        print(\"Model Learning\")\n",
    "        init = tf.global_variables_initializer()\n",
    "        sess = tf.Session()\n",
    "        e = self.start_e\n",
    "        current_step = 0 # previously total_steps\n",
    "\n",
    "        self.model_saver = ModelSaver(self.logs_dir)\n",
    "\n",
    "        with sess:\n",
    "\n",
    "            sess.run(init)\n",
    "           # if self.load_model == True:\n",
    "           #     print('Loading Model...')\n",
    "           #     model_saver.restore_model(sess)\n",
    "           #     #ckpt = model_saver_restore_model(sess)\n",
    "\n",
    "            # Set the target network to be equal to the primary network\n",
    "            self.update_target_graph(self.target_ops, sess)\n",
    "\n",
    "            # Start the pre train proces\n",
    "            for episode in range(self.num_episodes):\n",
    "\n",
    "                if episode % 100 == 0:\n",
    "                    print(\"\\n=====\" + \"Episode \" + str(episode) + \"starts =====\" )\n",
    "\n",
    "                episode_exp = ExperienceBuffer()\n",
    "\n",
    "                #Reset environment and get first new observation\n",
    "                s = self.env.reset()\n",
    "\n",
    "                d = False # episode's \"done\" signal\n",
    "                episode_reward_sum = 0\n",
    "                episode_steps = 0\n",
    "\n",
    "                #The Q-Network\n",
    "                while episode_steps < self.max_ep_length: #If the agent take too long to win, end the trial.\n",
    "                    episode_steps += 1\n",
    "\n",
    "                    #Choose an action by greedily (with e chance of random action) from the Q-network\n",
    "                    if np.random.rand(1) < e or current_step < self.pre_train_steps:\n",
    "                        a = np.random.randint(0,4)\n",
    "                    else:\n",
    "                        a = sess.run(self.mainQN.predict,feed_dict={self.mainQN.state:[s]})[0]\n",
    "\n",
    "                    s1,r,d = self.env.step(a)\n",
    "                    current_step += 1\n",
    "                    episode_exp.add(np.reshape(np.array([s,a,r,s1,d]),[1,5])) # Save the experience to our episode buffer.\n",
    "\n",
    "                    # Start train process\n",
    "                    if current_step > self.pre_train_steps:\n",
    "\n",
    "                        if e > self.final_e:\n",
    "                            stepDrop = 1/10000\n",
    "                            e -= stepDrop\n",
    "\n",
    "                        if current_step % self.update_freq == 0:\n",
    "\n",
    "                            train_batch = self.exp_buffer.sample(self.batch_size) #Get a random batch of experiences.\n",
    "\n",
    "                            #Perform the Double-DQN update to the target Q-values\n",
    "                            Q1 = sess.run(self.mainQN.predict,\n",
    "                                          feed_dict={self.mainQN.state:np.vstack(train_batch[:,3])})\n",
    "\n",
    "                            Q2 = sess.run(self.targetQN.Qout,\n",
    "                                          feed_dict={self.targetQN.state:np.vstack(train_batch[:,3])})\n",
    "\n",
    "                            end_multiplier = -(train_batch[:,4] - 1)\n",
    "                            doubleQ = Q2[range(self.batch_size),Q1]\n",
    "                            targetQ = train_batch[:,2] + (self.gamma*doubleQ*end_multiplier)\n",
    "\n",
    "                            # Update the network with our target values.\n",
    "                            _ = sess.run(self.mainQN.updateModel,\n",
    "                                         feed_dict={self.mainQN.state:np.vstack(train_batch[:,0]),\n",
    "                                         self.mainQN.targetQ:targetQ,\n",
    "                                         self.mainQN.actions:train_batch[:,1]})\n",
    "\n",
    "                            # Set the target network to be equal to the primary\n",
    "                            self.update_target_graph(self.target_ops, sess)\n",
    "\n",
    "                    episode_reward_sum += r\n",
    "                    s = s1\n",
    "\n",
    "                    if d == True:\n",
    "                        break\n",
    "\n",
    "                self.exp_buffer.add(episode_exp.buffer)\n",
    "                self.step_record.append(episode_steps)\n",
    "                self.reward_record.append(episode_reward_sum)\n",
    "\n",
    "                #Periodically save the model.\n",
    "                if episode % 1000 == 0:\n",
    "                    self.model_saver.save_model(sess, episode)\n",
    "                    print(\"Model save_model\")\n",
    "\n",
    "                if len(self.reward_record) % 10 == 0:\n",
    "                    print(current_step, np.mean(self.reward_record[-10:]), e)\n",
    "            ##\n",
    "            self.model_saver.save_model(sess, self.num_episodes)\n",
    "        print(\"Percent of succesful episodes: \" + str(100*sum(self.reward_record)/self.num_episodes) + \"%\")\n",
    "\n",
    "    \"\"\" Auxiliary Methods \"\"\"\n",
    "    # Originally called updateTargetGraph\n",
    "    def set_target_graph_vars(self, tfVars, tau):\n",
    "        total_vars = len(tfVars)\n",
    "        op_holder = []\n",
    "\n",
    "        for idx,var in enumerate(tfVars[0:total_vars//2]): # Select the first half of the variables (mainQ net)\n",
    "            op_holder.append(tfVars[idx+total_vars//2].assign((var.value()*tau)+((1-tau)*tfVars[idx+total_vars//2].value())))\n",
    "\n",
    "        return op_holder\n",
    "\n",
    "    \n",
    "    # Originally called updateTarget\n",
    "    def update_target_graph(self, op_holder, sess):\n",
    "        for op in op_holder:\n",
    "            sess.run(op)\n",
    "         \n",
    "    def test(self):\n",
    "        step_record_t = []\n",
    "        reward_record_t = []\n",
    "        current_steps_t = 0\n",
    "        \n",
    "        load_model = True\n",
    "        num_test_episodes = 3000\n",
    "        \n",
    "        \n",
    "        with tf.Session() as sess:\n",
    "            if load_model == True:\n",
    "                print('Loading Model...')\n",
    "                self.model_saver.restore_model(sess)\n",
    "                \n",
    "            for episode in range(num_test_episodes):\n",
    "                s = self.env.reset_for_testing()\n",
    "                d = False\n",
    "                episode_steps_t = 0\n",
    "                episode_reword_sum_t = 0\n",
    "\n",
    "                #Q-Network\n",
    "                while episode_steps_t < self.max_ep_length: \n",
    "                    episode_steps_t+=1\n",
    "                    a = sess.run(self.mainQN.predict, feed_dict={self.mainQN.state:[s]})[0]\n",
    "                    s1, r, d = self.env.step_for_testing(a)\n",
    "                    \n",
    "                    current_steps_t += 1\n",
    "                    episode_reword_sum_t += r\n",
    "                    s = s1\n",
    "                    \n",
    "                    if d == True:\n",
    "                        break\n",
    "                        \n",
    "                step_record_t.append(episode_steps_t)\n",
    "                reward_record_t.append(episode_reword_sum_t)\n",
    "\n",
    "                # print(episode_reword_sum_t)\n",
    "\n",
    "                if len(reward_record_t) % 10 == 0:\n",
    "                    accumR = np.mean(reward_record_t[-10:])\n",
    "                    log = str(current_steps_t) +\"\\t\"+ str(accumR) \n",
    "                    print(log)\n",
    "                    #f.write(log)\n",
    "                \n",
    "    \n",
    "        final_log = \"Percent of sucessful episodes: \" + str(sum(reward_record_t)/num_test_episodes) + \"%\"\n",
    "        print(final_log)\n",
    "        return reward_record_t\n",
    "        \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class DQN():\n",
    "    def __init__(self, net_name, state_size, action_size, hiddens, num_frames):\n",
    "        self.net_name = net_name\n",
    "\n",
    "        with tf.variable_scope(self.net_name):\n",
    "\n",
    "            #self.state = tf.placeholder(shape=[None, num_frames, state_size], dtype=tf.float32)\n",
    "            self.state = tf.placeholder(shape=[None, state_size], dtype=tf.float32)\n",
    "            #self.input_state = tf.reshape(self.state, [-1, num_frames * state_size])\n",
    "\n",
    "            # Weights of each layer\n",
    "            self.W = {\n",
    "                'W1': self.init_weight(\"W1\", [state_size, hiddens[0]]),\n",
    "                'W2': self.init_weight(\"W2\", [hiddens[0], hiddens[1]]),\n",
    "                'W3': self.init_weight(\"W3\", [hiddens[1], hiddens[2]]),\n",
    "                'AW': self.init_weight(\"AW\", [hiddens[2]//2, action_size]),\n",
    "               #'AW': self.init_weight(\"AW\", [hiddens[2]//2, hiddens[2]]),\n",
    "                'VM': self.init_weight(\"VM\", [hiddens[2]//2, 1])\n",
    "            }\n",
    "\n",
    "            self.b = {\n",
    "                'b1': self.init_bias(\"b1\", hiddens[0]),\n",
    "                'b2': self.init_bias(\"b2\", hiddens[1]),\n",
    "                'b3': self.init_bias(\"b3\", hiddens[2])\n",
    "            }\n",
    "\n",
    "            # Layers\n",
    "            self.hidden1 = tf.nn.relu(tf.add(tf.matmul(self.state, self.W['W1']), self.b['b1']))\n",
    "            self.hidden2 = tf.nn.relu(tf.add(tf.matmul(self.hidden1, self.W['W2']), self.b['b2']))\n",
    "            self.hidden3 = tf.nn.relu(tf.add(tf.matmul(self.hidden2, self.W['W3']), self.b['b3']))\n",
    "\n",
    "            # Compute the Advantage, Value, and total Q value\n",
    "            self.A, self.V = tf.split(self.hidden3, 2, 1)\n",
    "            self.Advantage = tf.matmul(self.A, self.W['AW'])\n",
    "            self.Value = tf.matmul(self.V, self.W['VM'])\n",
    "            self.Qout = self.Value + tf.subtract(self.Advantage, tf.reduce_mean(self.Advantage, axis=1, keep_dims=True))\n",
    "\n",
    "            # Calcultate the action with highest Q value\n",
    "            self.predict = tf.argmax(self.Qout, 1)\n",
    "\n",
    "            # Compute the loss (sum of squared differences)\n",
    "            self.targetQ = tf.placeholder(shape=[None], dtype=tf.float32)\n",
    "            self.actions = tf.placeholder(shape=[None], dtype=tf.int32)\n",
    "            self.actions_one_hot = tf.one_hot(self.actions, action_size, dtype=tf.float32)\n",
    "\n",
    "            self.Q = tf.reduce_sum(tf.multiply(self.Qout, self.actions_one_hot), axis=1)\n",
    "            self.td_error = tf.square(self.targetQ - self.Q)\n",
    "            self.loss = tf.reduce_mean(self.td_error)\n",
    "\n",
    "            self.trainer = tf.train.AdamOptimizer(learning_rate=0.0001)\n",
    "            self.updateModel = self.trainer.minimize(self.loss)\n",
    "\n",
    "    def init_weight(self, name, shape):\n",
    "        return tf.get_variable(name=name, shape=shape, initializer=initializer.xavier_initializer())\n",
    "                               #initializer=tf.contrib.layers.xavier_initializer())\n",
    "\n",
    "    def init_bias(self, name, shape):\n",
    "        return tf.Variable(tf.random_normal([shape]))\n",
    "        #return tf.get_variable(name=name, shape=shape, initializer=tf.constant(np.random.rand(range(shape))))\n",
    "                              # initializer= tf.random_initializer())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class ExperienceBuffer():\n",
    "    def __init__(self, buffer_size = 50000):\n",
    "        self.buffer = []\n",
    "        self.buffer_size = buffer_size\n",
    "\n",
    "    def add(self, experience):\n",
    "\n",
    "        if len(self.buffer) + len(experience) >= self.buffer_size:\n",
    "            self.buffer[0:(len(experience) + len(self.buffer)) - self.buffer_size] = []\n",
    "\n",
    "        self.buffer.extend(experience)\n",
    "\n",
    "    def sample(self, size):\n",
    "        return np.reshape(np.array(random.sample(self.buffer, size)),[size,5])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class ModelSaver():\n",
    "    def __init__(self, path):\n",
    "        self.saver = tf.train.Saver()\n",
    "        self.ckptPath = path\n",
    "        if not os.path.exists(path):\n",
    "            os.makedirs(path)\n",
    "\n",
    "    def restore_model(self, sess):\n",
    "        ckpt = tf.train.get_checkpoint_state(self.ckptPath)\n",
    "        self.saver.restore(sess, ckpt.model_checkpoint_path)\n",
    "\n",
    "    def save_model(self, sess, num_episode):\n",
    "        self.saver.save(sess, self.ckptPath+'/model-'+str(num_episode)+'.cptk')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class LogManager():\n",
    "    def __init__(self):\n",
    "        return\n",
    "\n",
    "    # If there is no file, create file with name firstly\n",
    "    # If there is file with name, append content in the file \n",
    "    def write(self, name=\"default_filename\", content=\"\"):\n",
    "        name = name+\".txt\"\n",
    "        with open(name, 'a') as f:\n",
    "            f.write(content)\n",
    "        return\n",
    "    def read(self, name=\"default_filename\"):\n",
    "        name = name+\".txt\"\n",
    "        try:\n",
    "            f = open(name, 'r')\n",
    "            data = f.read()\n",
    "            return data\n",
    "        except FileNotFoundError as e:\n",
    "            print(e)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class DrawGraph():\n",
    "    def __init__(self, reward_record):\n",
    "        self.reward_record = reward_record\n",
    "  \n",
    "    def plot(self):\n",
    "        rMat = np.resize(np.array(self.reward_record), [len(self.reward_record)//100,100])\n",
    "        rMean = np.average(rMat, 1)\n",
    "        plt.plot(rMean)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "qweqweqwewqeqwewqe11113432432\n"
     ]
    }
   ],
   "source": [
    "def main():\n",
    "    #env = gameEnv(partial = False, size=15)\n",
    "\n",
    "    #agent = Agent([256, 256, 256], 4, 10, 6, env, 1e-3, 32, 10000, 100, 50000,\n",
    "    #              1, 0.01, 10000, 0.99, 0.001, 5, False, \"./log_dir\")\n",
    "\n",
    "    # The parameters of Agent class are as follows:\n",
    "    # hiddens, num_frames, state_size, action_size, env, lr, batch_size, num_episodes, pre_train_steps,\n",
    "    # max_ep_length, buffer_size, start_e, final_e, gamma, tau, update_freq, load_model, logs_dir\n",
    "    \n",
    "    name=\"test\"\n",
    "    log = LogManager()\n",
    "    log.write(name, \"13432432\")\n",
    "    print(log.read(name))\n",
    "    \n",
    "    \"\"\"\n",
    "    agent = Agent([256, 256, 256], 4, 10, 6, env, 1e-3, 32, 10000, 10000, 500, 50000, 1, 0.01, 0.99, 0.001, 5, False, \"./logs_dir\")\n",
    "    agent.learn()\n",
    "    \n",
    "    reward_record_t = agent.test()\n",
    "    graph = DrawGraph(reward_record_t)\n",
    "    graph.plot()\n",
    "    \"\"\"\n",
    "    \n",
    "if __name__ == '__main__':\n",
    "    main()\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
