{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from __future__ import division \n",
    "\n",
    "import gym\n",
    "import numpy as np\n",
    "import random\n",
    "import tensorflow as tf \n",
    "import tensorflow.contrib.slim as slim\n",
    "import tensorflow.contrib.layers as initializer\n",
    "import scipy.misc\n",
    "import os\n",
    "import sys\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from gridworld_v04 import gameEnv\n",
    "#env = gameEnv(partial=False, size=15)\n",
    "#env.renderEnv()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class Agent():\n",
    "    def __init__(self, hiddens = [256, 256, 256], num_frames = 4, state_size = 10,\n",
    "                 action_size = 6, env=None, lr = 1e-3, batch_size = 32, num_episodes = 10000, pre_train_steps = 10000,\n",
    "                 max_ep_length = 500, buffer_size = 50000, start_e = 1, final_e = 0.01, gamma = 0.99,\n",
    "                 tau=0.001, update_freq = 5, load_model = False , logs_dir = \"./logs_dir\"):\n",
    "\n",
    "        self.hiddens = hiddens # Size of the hidden layers\n",
    "        self.num_frames = num_frames # Number of consecutive state frames\n",
    "        self.state_size = state_size # Size of the state vector\n",
    "        self.action_size = action_size  # Number of actions\n",
    "        self.env = env # Save the environmet!\n",
    "        self.lr=lr # learning rate of the optimizer\n",
    "        self.batch_size=batch_size # Size of the experience sample batch\n",
    "        self.num_episodes = num_episodes # Number of game environmet episodes in which we train\n",
    "        self.pre_train_steps = pre_train_steps\n",
    "        self.max_ep_length = max_ep_length # Maximun length (number of actions) of a single train episode\n",
    "        self.buffer_size = buffer_size # Size of the experience replay buffer\n",
    "        self.start_e = start_e # Inital value of the exploration coefficient\n",
    "        self.final_e = final_e # Final value of the exploration coefficient\n",
    "        self.gamma = gamma # Discount factor on the target Q-value\n",
    "        self.tau = tau # Porcentage that determines how much are parameters of mainQN net modified by targetQN\n",
    "        self.update_freq = update_freq # Frecuency of updates of the double DQN\n",
    "        self.logs_dir = logs_dir # Path to store logs and checkpoints\n",
    "\n",
    "        tf.reset_default_graph()\n",
    "\n",
    "        # Instantiate the networks\n",
    "        #self.mainQN = DQN(\"mainQN\", state_size, action_size, hiddens, num_frames)\n",
    "        self.mainQN = DQN(\"mainQN\", len(self.env.objects) * 2, self.env.actions, hiddens, num_frames)\n",
    "        self.targetQN = DQN(\"targetQN\", state_size, action_size, hiddens, num_frames)\n",
    "\n",
    "        init = tf.global_variables_initializer()\n",
    "        trainables = tf.trainable_variables()\n",
    "\n",
    "        self.target_ops = self.set_target_graph_vars(trainables, tau)\n",
    "\n",
    "        # Create a experience replay buffer & score records\n",
    "        self.exp_buffer = ExperienceBuffer()\n",
    "        self.step_record = []\n",
    "        self.reward_record = []\n",
    "\n",
    "        \n",
    "    def learn(self):\n",
    "        print(\"Model Learning\")\n",
    "        init = tf.global_variables_initializer()\n",
    "        sess = tf.Session()\n",
    "        e = self.start_e\n",
    "        current_step = 0 # previously total_steps\n",
    "\n",
    "        model_saver = ModelSaver(self.logs_dir)\n",
    "\n",
    "        with sess:\n",
    "\n",
    "            sess.run(init)\n",
    "           # if self.load_model == True:\n",
    "           #     print('Loading Model...')\n",
    "           #     model_saver.restore_model(sess)\n",
    "           #     #ckpt = model_saver_restore_model(sess)\n",
    "\n",
    "            # Set the target network to be equal to the primary network\n",
    "            self.update_target_graph(self.target_ops, sess)\n",
    "\n",
    "            # Start the pre train proces\n",
    "            for episode in range(self.num_episodes):\n",
    "\n",
    "                if episode % 100 == 0:\n",
    "                    print(\"\\n=====\" + \"Episode \" + str(episode) + \"starts =====\" )\n",
    "\n",
    "                episode_exp = ExperienceBuffer()\n",
    "\n",
    "                #Reset environment and get first new observation\n",
    "                s = self.env.reset()\n",
    "\n",
    "                d = False # episode's \"done\" signal\n",
    "                episode_reward_sum = 0\n",
    "                episode_steps = 0\n",
    "\n",
    "                #The Q-Network\n",
    "                while episode_steps < self.max_ep_length: #If the agent take too long to win, end the trial.\n",
    "                    episode_steps += 1\n",
    "\n",
    "                    #Choose an action by greedily (with e chance of random action) from the Q-network\n",
    "                    if np.random.rand(1) < e or current_step < self.pre_train_steps:\n",
    "                        a = np.random.randint(0,4)\n",
    "                    else:\n",
    "                        a = sess.run(self.mainQN.predict,feed_dict={mainQN.state:[s]})[0]\n",
    "\n",
    "                    s1,r,d = self.env.step(a)\n",
    "                    current_step += 1\n",
    "                    episode_exp.add(np.reshape(np.array([s,a,r,s1,d]),[1,5])) # Save the experience to our episode buffer.\n",
    "\n",
    "                    # Start train process\n",
    "                    if current_step > self.pre_train_steps:\n",
    "\n",
    "                        if e > self.final_e:\n",
    "                            stepDrop = 1/10000\n",
    "                            e -= stepDrop\n",
    "\n",
    "                        if current_step % self.update_freq == 0:\n",
    "\n",
    "                            train_batch = self.exp_buffer.sample(self.batch_size) #Get a random batch of experiences.\n",
    "\n",
    "                            #Perform the Double-DQN update to the target Q-values\n",
    "                            Q1 = sess.run(self.mainQN.predict,\n",
    "                                          feed_dict={self.mainQN.state:np.vstack(train_batch[:,3])})\n",
    "\n",
    "                            Q2 = sess.run(self.targetQN.Qout,\n",
    "                                          feed_dict={self.targetQN.state:np.vstack(train_batch[:,3])})\n",
    "\n",
    "                            end_multiplier = -(train_batch[:,4] - 1)\n",
    "                            doubleQ = Q2[range(self.batch_size),Q1]\n",
    "                            targetQ = train_batch[:,2] + (self.gamma*doubleQ*end_multiplier)\n",
    "\n",
    "                            # Update the network with our target values.\n",
    "                            _ = sess.run(self.mainQN.updateModel,\n",
    "                                         feed_dict={self.mainQN.state:np.vstack(train_batch[:,0]),\n",
    "                                         self.mainQN.targetQ:targetQ,\n",
    "                                         self.mainQN.actions:train_batch[:,1]})\n",
    "\n",
    "                            # Set the target network to be equal to the primary\n",
    "                            self.update_target_graph(self.target_ops, sess)\n",
    "\n",
    "                    episode_reward_sum += r\n",
    "                    s = s1\n",
    "\n",
    "                    if d == True:\n",
    "                        break\n",
    "\n",
    "                self.exp_buffer.add(episode_exp.buffer)\n",
    "                self.step_record.append(episode_steps)\n",
    "                self.reward_record.append(episode_reward_sum)\n",
    "\n",
    "                #Periodically save the model.\n",
    "                if episode % 1000 == 0:\n",
    "                    model_saver.save_model(sess, episode)\n",
    "                    print(\"Model save_model\")\n",
    "\n",
    "                if len(self.reward_record) % 10 == 0:\n",
    "                    print(current_step, np.mean(self.reward_record[-10:]), e)\n",
    "            ##\n",
    "            model_saver.save_model(sess, self.num_episodes)\n",
    "        print(\"Percent of succesful episodes: \" + str(100*sum(self.reward_record)/self.num_episodes) + \"%\")\n",
    "\n",
    "    \"\"\" Auxiliary Methods \"\"\"\n",
    "    # Originally called updateTargetGraph\n",
    "    def set_target_graph_vars(self, tfVars, tau):\n",
    "        total_vars = len(tfVars)\n",
    "        op_holder = []\n",
    "\n",
    "        for idx,var in enumerate(tfVars[0:total_vars//2]): # Select the first half of the variables (mainQ net)\n",
    "            op_holder.append(tfVars[idx+total_vars//2].assign((var.value()*tau)+((1-tau)*tfVars[idx+total_vars//2].value())))\n",
    "\n",
    "        return op_holder\n",
    "\n",
    "    \n",
    "    # Originally called updateTarget\n",
    "    def update_target_graph(self, op_holder, sess):\n",
    "        for op in op_holder:\n",
    "            sess.run(op)\n",
    "         \n",
    "    return op_holder\n",
    "            \n",
    "    \n",
    "    def test(self):\n",
    "        step_record_t = []\n",
    "        reward_record_t = []\n",
    "        current_steps_t = 0\n",
    "        \n",
    "        load_model = True\n",
    "        num_test_episodes = 5000\n",
    "        \n",
    "        model_saver = ModelSaver(self.logs_dir)\n",
    "        \n",
    "        with tf.Session() as sess:\n",
    "            if load_model == True:\n",
    "                print('Loading Model...')\n",
    "                model_saver.restore_model(sess)\n",
    "                \n",
    "            for episode in range(num_test_episodes):\n",
    "                s = env.reset_for_testing()\n",
    "                d = False\n",
    "                episode_steps_t = 0\n",
    "                episode_reword_sum_t = 0\n",
    "\n",
    "                #Q-Network\n",
    "                while episode_steps_t < self.max_ep_length: \n",
    "                    episode_steps_t+=1\n",
    "                    a = sess.run(mainQN.predict, feed_dict={mainQN.input:[s]})[0]\n",
    "                    s1, r, d = env.step_for_testing(a)\n",
    "                    \n",
    "                    current_steps_t += 1\n",
    "                    episode_reword_sum_t += r\n",
    "                    s = s1\n",
    "                    \n",
    "                    if d == True:\n",
    "                        break\n",
    "                        \n",
    "        step_record_t.append(episode_steps_t)\n",
    "        reward_record_t.append(episode_reword_sum_t)\n",
    "            \n",
    "        # print(episode_reword_sum_t)\n",
    "        \n",
    "        if len(reward_record_t) % 10 == 0:\n",
    "            accumR = np.mean(reward_record_t[-10:])\n",
    "            log = str(current_steps_t) +\"\\t\"+ str(accumR) \n",
    "            print(log)\n",
    "            #f.write(log)\n",
    "                \n",
    "    \n",
    "        final_log = \"Percent of sucessful episodes: \" + str(sum(reward_record_t)/num_test_episodes) + \"%\"\n",
    "        print(final_log)\n",
    "        \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class DQN():\n",
    "    def __init__(self, net_name, state_size, action_size, hiddens, num_frames):\n",
    "        self.net_name = net_name\n",
    "\n",
    "        with tf.variable_scope(self.net_name):\n",
    "\n",
    "            #self.state = tf.placeholder(shape=[None, num_frames, state_size], dtype=tf.float32)\n",
    "            self.state = tf.placeholder(shape=[None, state_size*2], dtype=tf.float32)\n",
    "            #self.input_state = tf.reshape(self.state, [-1, num_frames * state_size])\n",
    "\n",
    "            # Weights of each layer\n",
    "            self.W = {\n",
    "                'W1': self.init_weight(\"W1\", [state_size*2, hiddens[0]]),\n",
    "                'W2': self.init_weight(\"W2\", [hiddens[0], hiddens[1]]),\n",
    "                'W3': self.init_weight(\"W3\", [hiddens[1], hiddens[2]]),\n",
    "                'AW': self.init_weight(\"AW\", [hiddens[2]//2, action_size]),\n",
    "               #'AW': self.init_weight(\"AW\", [hiddens[2]//2, hiddens[2]]),\n",
    "                'VM': self.init_weight(\"VM\", [hiddens[2]//2, 1])\n",
    "            }\n",
    "\n",
    "            self.b = {\n",
    "                'b1': self.init_bias(\"b1\", hiddens[0]),\n",
    "                'b2': self.init_bias(\"b2\", hiddens[1]),\n",
    "                'b3': self.init_bias(\"b3\", hiddens[2])\n",
    "            }\n",
    "\n",
    "            # Layers\n",
    "            self.hidden1 = tf.nn.relu(tf.add(tf.matmul(self.state, self.W['W1']), self.b['b1']))\n",
    "            self.hidden2 = tf.nn.relu(tf.add(tf.matmul(self.hidden1, self.W['W2']), self.b['b2']))\n",
    "            self.hidden3 = tf.nn.relu(tf.add(tf.matmul(self.hidden2, self.W['W3']), self.b['b3']))\n",
    "\n",
    "            # Compute the Advantage, Value, and total Q value\n",
    "            self.A, self.V = tf.split(self.hidden3, 2, 1)\n",
    "            self.Advantage = tf.matmul(self.A, self.W['AW'])\n",
    "            self.Value = tf.matmul(self.V, self.W['VM'])\n",
    "            self.Qout = self.Value + tf.subtract(self.Advantage, tf.reduce_mean(self.Advantage, axis=1, keep_dims=True))\n",
    "\n",
    "            # Calcultate the action with highest Q value\n",
    "            self.predict = tf.argmax(self.Qout, 1)\n",
    "\n",
    "            # Compute the loss (sum of squared differences)\n",
    "            self.targetQ = tf.placeholder(shape=[None], dtype=tf.float32)\n",
    "            self.actions = tf.placeholder(shape=[None], dtype=tf.int32)\n",
    "            self.actions_one_hot = tf.one_hot(self.actions, action_size, dtype=tf.float32)\n",
    "\n",
    "            self.Q = tf.reduce_sum(tf.multiply(self.Qout, self.actions_one_hot), axis=1)\n",
    "            self.td_error = tf.square(self.targetQ - self.Q)\n",
    "            self.loss = tf.reduce_mean(self.td_error)\n",
    "\n",
    "            self.trainer = tf.train.AdamOptimizer(learning_rate=0.0001)\n",
    "            self.updateModel = self.trainer.minimize(self.loss)\n",
    "\n",
    "    def init_weight(self, name, shape):\n",
    "        return tf.get_variable(name=name, shape=shape, initializer=initializer.xavier_initializer())\n",
    "                               #initializer=tf.contrib.layers.xavier_initializer())\n",
    "\n",
    "    def init_bias(self, name, shape):\n",
    "        return tf.Variable(tf.random_normal([shape]))\n",
    "        #return tf.get_variable(name=name, shape=shape, initializer=tf.constant(np.random.rand(range(shape))))\n",
    "                              # initializer= tf.random_initializer())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class ExperienceBuffer():\n",
    "    def __init__(self, buffer_size = 50000):\n",
    "        self.buffer = []\n",
    "        self.buffer_size = buffer_size\n",
    "\n",
    "    def add(self, experience):\n",
    "\n",
    "        if len(self.buffer) + len(experience) >= self.buffer_size:\n",
    "            self.buffer[0:(len(experience) + len(self.buffer)) - self.buffer_size] = []\n",
    "\n",
    "        self.buffer.extend(experience)\n",
    "\n",
    "    def sample(self, size):\n",
    "        return np.reshape(np.array(random.sample(self.buffer, size)),[size,5])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class ModelSaver():\n",
    "    def __init__(self, path):\n",
    "        self.saver = tf.train.Saver()\n",
    "        self.ckptPath = path\n",
    "        if not os.path.exists(path):\n",
    "            os.makedirs(path)\n",
    "\n",
    "    def restore_model(self, sess):\n",
    "        ckpt = tf.train.get_checkpoint_state(self.ckptPath)\n",
    "        self.saver.restore(sess, ckpt.model_checkpoint_path)\n",
    "\n",
    "    def save_model(self, sess, num_episode):\n",
    "        self.saver.save(sess, self.ckptPath+'/model-'+str(num_episode)+'.cptk')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "Dimensions must be equal, but are 56 and 20 for 'add' (op: 'Add') with input shapes: [56,256], [20,256].",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mInvalidArgumentError\u001b[0m                      Traceback (most recent call last)",
      "\u001b[0;32m/usr/local/lib/python3.4/dist-packages/tensorflow/python/framework/common_shapes.py\u001b[0m in \u001b[0;36m_call_cpp_shape_fn_impl\u001b[0;34m(op, input_tensors_needed, input_tensors_as_shapes_needed, debug_python_shape_fn, require_shape_fn)\u001b[0m\n\u001b[1;32m    670\u001b[0m           \u001b[0mgraph_def_version\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnode_def_str\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput_shapes\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput_tensors\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 671\u001b[0;31m           input_tensors_as_shapes, status)\n\u001b[0m\u001b[1;32m    672\u001b[0m   \u001b[0;32mexcept\u001b[0m \u001b[0merrors\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mInvalidArgumentError\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0merr\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/lib/python3.4/contextlib.py\u001b[0m in \u001b[0;36m__exit__\u001b[0;34m(self, type, value, traceback)\u001b[0m\n\u001b[1;32m     65\u001b[0m             \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 66\u001b[0;31m                 \u001b[0mnext\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgen\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     67\u001b[0m             \u001b[0;32mexcept\u001b[0m \u001b[0mStopIteration\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.4/dist-packages/tensorflow/python/framework/errors_impl.py\u001b[0m in \u001b[0;36mraise_exception_on_not_ok_status\u001b[0;34m()\u001b[0m\n\u001b[1;32m    465\u001b[0m           \u001b[0mcompat\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mas_text\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpywrap_tensorflow\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTF_Message\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstatus\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 466\u001b[0;31m           pywrap_tensorflow.TF_GetCode(status))\n\u001b[0m\u001b[1;32m    467\u001b[0m   \u001b[0;32mfinally\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mInvalidArgumentError\u001b[0m: Dimensions must be equal, but are 56 and 20 for 'add' (op: 'Add') with input shapes: [56,256], [20,256].",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-16-e8a43272d4c2>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     12\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     13\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0m__name__\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m'__main__'\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 14\u001b[0;31m     \u001b[0mmain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m<ipython-input-16-e8a43272d4c2>\u001b[0m in \u001b[0;36mmain\u001b[0;34m()\u001b[0m\n\u001b[1;32m      8\u001b[0m     \u001b[0;31m# hiddens, num_frames, state_size, action_size, env, lr, batch_size, num_episodes, pre_train_steps,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      9\u001b[0m     \u001b[0;31m# max_ep_length, buffer_size, start_e, final_e, gamma, tau, update_freq, load_model, logs_dir\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 10\u001b[0;31m     \u001b[0magent\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mAgent\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m256\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m256\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m256\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m4\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m10\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m6\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0menv\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m1e-3\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m32\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m10000\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m10000\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m500\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m50000\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m0.01\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m0.99\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m0.001\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m5\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;32mFalse\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"./logs_dir\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     11\u001b[0m     \u001b[0magent\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlearn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     12\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-12-95c136726bbe>\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, hiddens, num_frames, state_size, action_size, env, lr, batch_size, num_episodes, pre_train_steps, max_ep_length, buffer_size, start_e, final_e, gamma, tau, update_freq, load_model, logs_dir)\u001b[0m\n\u001b[1;32m     33\u001b[0m         \u001b[0mtrainables\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrainable_variables\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     34\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 35\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtarget_ops\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mset_target_graph_vars\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrainables\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtau\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     36\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     37\u001b[0m         \u001b[0;31m# Create a experience replay buffer & score records\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-12-95c136726bbe>\u001b[0m in \u001b[0;36mset_target_graph_vars\u001b[0;34m(self, tfVars, tau)\u001b[0m\n\u001b[1;32m    149\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    150\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0midx\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mvar\u001b[0m \u001b[0;32min\u001b[0m \u001b[0menumerate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtfVars\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0mtotal_vars\u001b[0m\u001b[0;34m//\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0;31m# Select the first half of the variables (mainQ net)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 151\u001b[0;31m             \u001b[0mop_holder\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m \u001b[0mtfVars\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0midx\u001b[0m\u001b[0;34m+\u001b[0m\u001b[0mtotal_vars\u001b[0m\u001b[0;34m//\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0massign\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mvar\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalue\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0mtau\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m+\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0mtau\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0mtfVars\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0midx\u001b[0m\u001b[0;34m+\u001b[0m\u001b[0mtotal_vars\u001b[0m\u001b[0;34m//\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalue\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    152\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    153\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mop_holder\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.4/dist-packages/tensorflow/python/ops/math_ops.py\u001b[0m in \u001b[0;36mbinary_op_wrapper\u001b[0;34m(x, y)\u001b[0m\n\u001b[1;32m    819\u001b[0m       \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msparse_tensor\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mSparseTensor\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    820\u001b[0m         \u001b[0my\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mops\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mconvert_to_tensor\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdtype\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbase_dtype\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mname\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"y\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 821\u001b[0;31m       \u001b[0;32mreturn\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mname\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    822\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    823\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0mbinary_op_wrapper_sparse\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msp_x\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.4/dist-packages/tensorflow/python/ops/gen_math_ops.py\u001b[0m in \u001b[0;36madd\u001b[0;34m(x, y, name)\u001b[0m\n\u001b[1;32m     71\u001b[0m     \u001b[0mA\u001b[0m\u001b[0;31m \u001b[0m\u001b[0;31m`\u001b[0m\u001b[0mTensor\u001b[0m\u001b[0;31m`\u001b[0m\u001b[0;34m.\u001b[0m \u001b[0mHas\u001b[0m \u001b[0mthe\u001b[0m \u001b[0msame\u001b[0m \u001b[0mtype\u001b[0m \u001b[0;32mas\u001b[0m\u001b[0;31m \u001b[0m\u001b[0;31m`\u001b[0m\u001b[0mx\u001b[0m\u001b[0;31m`\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     72\u001b[0m   \"\"\"\n\u001b[0;32m---> 73\u001b[0;31m   \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_op_def_lib\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mapply_op\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Add\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0my\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mname\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     74\u001b[0m   \u001b[0;32mreturn\u001b[0m \u001b[0mresult\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     75\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.4/dist-packages/tensorflow/python/framework/op_def_library.py\u001b[0m in \u001b[0;36mapply_op\u001b[0;34m(self, op_type_name, name, **keywords)\u001b[0m\n\u001b[1;32m    766\u001b[0m         op = g.create_op(op_type_name, inputs, output_types, name=scope,\n\u001b[1;32m    767\u001b[0m                          \u001b[0minput_types\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0minput_types\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mattrs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mattr_protos\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 768\u001b[0;31m                          op_def=op_def)\n\u001b[0m\u001b[1;32m    769\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0moutput_structure\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    770\u001b[0m           \u001b[0moutputs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mop\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moutputs\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.4/dist-packages/tensorflow/python/framework/ops.py\u001b[0m in \u001b[0;36mcreate_op\u001b[0;34m(self, op_type, inputs, dtypes, input_types, name, attrs, op_def, compute_shapes, compute_device)\u001b[0m\n\u001b[1;32m   2336\u001b[0m                     original_op=self._default_original_op, op_def=op_def)\n\u001b[1;32m   2337\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mcompute_shapes\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2338\u001b[0;31m       \u001b[0mset_shapes_for_outputs\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mret\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2339\u001b[0m     \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_add_op\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mret\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2340\u001b[0m     \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_record_op_seen_by_control_dependencies\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mret\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.4/dist-packages/tensorflow/python/framework/ops.py\u001b[0m in \u001b[0;36mset_shapes_for_outputs\u001b[0;34m(op)\u001b[0m\n\u001b[1;32m   1717\u001b[0m       \u001b[0mshape_func\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_call_cpp_shape_fn_and_require_op\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1718\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1719\u001b[0;31m   \u001b[0mshapes\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mshape_func\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mop\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1720\u001b[0m   \u001b[0;32mif\u001b[0m \u001b[0mshapes\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1721\u001b[0m     raise RuntimeError(\n",
      "\u001b[0;32m/usr/local/lib/python3.4/dist-packages/tensorflow/python/framework/ops.py\u001b[0m in \u001b[0;36mcall_with_requiring\u001b[0;34m(op)\u001b[0m\n\u001b[1;32m   1667\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1668\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0mcall_with_requiring\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mop\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1669\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0mcall_cpp_shape_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mop\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrequire_shape_fn\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1670\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1671\u001b[0m   \u001b[0m_call_cpp_shape_fn_and_require_op\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcall_with_requiring\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.4/dist-packages/tensorflow/python/framework/common_shapes.py\u001b[0m in \u001b[0;36mcall_cpp_shape_fn\u001b[0;34m(op, input_tensors_needed, input_tensors_as_shapes_needed, debug_python_shape_fn, require_shape_fn)\u001b[0m\n\u001b[1;32m    608\u001b[0m     res = _call_cpp_shape_fn_impl(op, input_tensors_needed,\n\u001b[1;32m    609\u001b[0m                                   \u001b[0minput_tensors_as_shapes_needed\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 610\u001b[0;31m                                   debug_python_shape_fn, require_shape_fn)\n\u001b[0m\u001b[1;32m    611\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mres\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdict\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    612\u001b[0m       \u001b[0;31m# Handles the case where _call_cpp_shape_fn_impl calls unknown_shape(op).\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.4/dist-packages/tensorflow/python/framework/common_shapes.py\u001b[0m in \u001b[0;36m_call_cpp_shape_fn_impl\u001b[0;34m(op, input_tensors_needed, input_tensors_as_shapes_needed, debug_python_shape_fn, require_shape_fn)\u001b[0m\n\u001b[1;32m    674\u001b[0m       \u001b[0mmissing_shape_fn\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    675\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 676\u001b[0;31m       \u001b[0;32mraise\u001b[0m \u001b[0mValueError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0merr\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmessage\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    677\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    678\u001b[0m   \u001b[0;32mif\u001b[0m \u001b[0mmissing_shape_fn\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mValueError\u001b[0m: Dimensions must be equal, but are 56 and 20 for 'add' (op: 'Add') with input shapes: [56,256], [20,256]."
     ]
    }
   ],
   "source": [
    "def main():\n",
    "    env = gameEnv(partial = False, size=15)\n",
    "\n",
    "    #agent = Agent([256, 256, 256], 4, 10, 6, env, 1e-3, 32, 10000, 100, 50000,\n",
    "    #              1, 0.01, 10000, 0.99, 0.001, 5, False, \"./log_dir\")\n",
    "\n",
    "    # The parameters of Agent class are as follows:\n",
    "    # hiddens, num_frames, state_size, action_size, env, lr, batch_size, num_episodes, pre_train_steps,\n",
    "    # max_ep_length, buffer_size, start_e, final_e, gamma, tau, update_freq, load_model, logs_dir\n",
    "    agent = Agent([256, 256, 256], 4, 10, 6, env, 1e-3, 32, 10000, 10000, 500, 50000, 1, 0.01, 0.99, 0.001, 5, False, \"./logs_dir\")\n",
    "    agent.learn()\n",
    "    \n",
    "if __name__ == '__main__':\n",
    "    main()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.4.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
