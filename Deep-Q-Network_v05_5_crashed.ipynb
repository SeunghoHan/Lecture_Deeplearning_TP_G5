{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from __future__ import division \n",
    "\n",
    "import gym\n",
    "import numpy as np\n",
    "import random\n",
    "import tensorflow as tf \n",
    "import tensorflow.contrib.slim as slim\n",
    "import tensorflow.contrib.layers as initializer\n",
    "import matplotlib.pyplot as plt\n",
    "import scipy.misc\n",
    "import os\n",
    "import sys\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from gridworld_v04 import gameEnv\n",
    "#env = gameEnv(partial=False, size=15)\n",
    "#env.renderEnv()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class Agent():\n",
    "    def __init__(self, hiddens = [256, 256, 256], num_frames = 4, state_size = 10,\n",
    "                 action_size = 6, env=None, lr = 1e-3, batch_size = 32, num_episodes = 5000, pre_train_steps = 10000,\n",
    "                 max_ep_length = 300, buffer_size = 50000, start_e = 1, final_e = 0.01, gamma = 0.99,\n",
    "                 tau=0.001, update_freq = 5, load_model = False , logs_dir = \"./logs_dir\"):\n",
    "\n",
    "        self.hiddens = hiddens # Size of the hidden layers\n",
    "        self.num_frames = num_frames # Number of consecutive state frames\n",
    "        self.state_size = state_size # Size of the state vector\n",
    "        self.action_size = action_size  # Number of actions\n",
    "        self.env = env # Save the environmet!\n",
    "        self.lr=lr # learning rate of the optimizer\n",
    "        self.batch_size=batch_size # Size of the experience sample batch\n",
    "        self.num_episodes = num_episodes # Number of game environmet episodes in which we train\n",
    "        self.pre_train_steps = pre_train_steps\n",
    "        self.max_ep_length = max_ep_length # Maximun length (number of actions) of a single train episode\n",
    "        self.buffer_size = buffer_size # Size of the experience replay buffer\n",
    "        self.start_e = start_e # Inital value of the exploration coefficient\n",
    "        self.final_e = final_e # Final value of the exploration coefficient\n",
    "        self.gamma = gamma # Discount factor on the target Q-value\n",
    "        self.tau = tau # Porcentage that determines how much are parameters of mainQN net modified by targetQN\n",
    "        self.update_freq = update_freq # Frecuency of updates of the double DQN\n",
    "        self.logs_dir = logs_dir # Path to store logs and checkpoints\n",
    "\n",
    "        tf.reset_default_graph()\n",
    "\n",
    "        # Instantiate the networks\n",
    "        #self.mainQN = DQN(\"mainQN\", state_size, action_size, hiddens, num_frames)\n",
    "        self.mainQN = DQN(\"mainQN\", len(self.env.objects) * 2, self.env.actions, hiddens, num_frames)\n",
    "        self.targetQN = DQN(\"targetQN\", len(self.env.objects) * 2, self.env.actions, hiddens, num_frames)\n",
    "\n",
    "        init = tf.global_variables_initializer()\n",
    "        trainables = tf.trainable_variables()\n",
    "\n",
    "        self.target_ops = self.set_target_graph_vars(trainables, tau)\n",
    "\n",
    "        # Create a experience replay buffer & score records\n",
    "        self.exp_buffer = ExperienceBuffer()\n",
    "        self.step_record = []\n",
    "        self.reward_record = []\n",
    "\n",
    "        self.model_saver = ModelSaver(self.logs_dir)\n",
    "        self.file_manager = FileManager()\n",
    "        self.file_name_train = \"data_log_for_training\"\n",
    "        self.file_name_test = \"data_log_for_testing\"\n",
    "        \n",
    "    def learn(self):\n",
    "        print(\"Model Learning\")\n",
    "        init = tf.global_variables_initializer()\n",
    "        sess = tf.Session()\n",
    "        e = self.start_e\n",
    "        current_step = 0 # previously total_steps\n",
    "        \n",
    "        horizontal_delimiter = \"\"\n",
    "        for _ in range(20):\n",
    "            horizontal_delimiter += \"-\"\n",
    "\n",
    "        self.model_saver = ModelSaver(self.logs_dir)\n",
    "\n",
    "        with sess:\n",
    "\n",
    "            sess.run(init)\n",
    "            # if self.load_model == True:\n",
    "            #     print('Loading Model...')\n",
    "            #     model_saver.restore_model(sess)\n",
    "            #     #ckpt = model_saver_restore_model(sess)\n",
    "\n",
    "            # Set the target network to be equal to the primary network\n",
    "            self.update_target_graph(self.target_ops, sess)\n",
    "\n",
    "            # Start the pre train proces\n",
    "            for episode in range(self.num_episodes):\n",
    "\n",
    "                if episode % 100 == 0:\n",
    "                    print(\"\\n=====\" + \"Episode \" + str(episode) + \"starts =====\" )\n",
    "                \n",
    "                self.file_manager.write(self.file_name_train, \"episode {0}\".format(episode+1))\n",
    "\n",
    "                episode_exp = ExperienceBuffer()\n",
    "\n",
    "                #Reset environment and get first new observation\n",
    "                s = self.env.reset()\n",
    "\n",
    "                d = False # episode's \"done\" signal\n",
    "                episode_reward_sum = 0\n",
    "                episode_steps = 0\n",
    "\n",
    "                #The Q-Network\n",
    "                while episode_steps < self.max_ep_length: #If the agent take too long to win, end the trial.\n",
    "                    episode_steps += 1\n",
    "\n",
    "                    #Choose an action by greedily (with e chance of random action) from the Q-network\n",
    "                    if np.random.rand(1) < e or current_step < self.pre_train_steps:\n",
    "                        a = np.random.randint(0,4)\n",
    "                    else:\n",
    "                        a = sess.run(self.mainQN.predict,feed_dict={self.mainQN.state:[s]})[0]\n",
    "\n",
    "                    s1,r,d = self.env.step(a)\n",
    "                    current_step += 1\n",
    "                    episode_exp.add(np.reshape(np.array([s,a,r,s1,d]),[1,5])) # Save the experience to our episode buffer.\n",
    "\n",
    "                    # Start train process\n",
    "                    if current_step > self.pre_train_steps:\n",
    "\n",
    "                        if e > self.final_e:\n",
    "                            stepDrop = 1/10000\n",
    "                            e -= stepDrop\n",
    "\n",
    "                        if current_step % self.update_freq == 0:\n",
    "\n",
    "                            train_batch = self.exp_buffer.sample(self.batch_size) #Get a random batch of experiences.\n",
    "\n",
    "                            #Perform the Double-DQN update to the target Q-values\n",
    "                            Q1 = sess.run(self.mainQN.predict,\n",
    "                                          feed_dict={self.mainQN.state:np.vstack(train_batch[:,3])})\n",
    "\n",
    "                            Q2 = sess.run(self.targetQN.Qout,\n",
    "                                          feed_dict={self.targetQN.state:np.vstack(train_batch[:,3])})\n",
    "\n",
    "                            end_multiplier = -(train_batch[:,4] - 1)\n",
    "                            doubleQ = Q2[range(self.batch_size),Q1]\n",
    "                            targetQ = train_batch[:,2] + (self.gamma*doubleQ*end_multiplier)\n",
    "\n",
    "                            # Update the network with our target values.\n",
    "                            _ = sess.run(self.mainQN.updateModel,\n",
    "                                         feed_dict={self.mainQN.state:np.vstack(train_batch[:,0]),\n",
    "                                         self.mainQN.targetQ:targetQ,\n",
    "                                         self.mainQN.actions:train_batch[:,1]})\n",
    "\n",
    "                            # Set the target network to be equal to the primary\n",
    "                            self.update_target_graph(self.target_ops, sess)\n",
    "\n",
    "                    episode_reward_sum += r\n",
    "                    s = s1\n",
    "\n",
    "                    if d == True:\n",
    "                        break\n",
    "\n",
    "                self.exp_buffer.add(episode_exp.buffer)\n",
    "                self.step_record.append(episode_steps)\n",
    "                self.reward_record.append(episode_reward_sum)\n",
    "                \n",
    "                #self.file_manager.write(self.file_name_train, \"step_record:\".join(self.step_record))\n",
    "                self.file_manager.write(self.file_name_train, \"step_record: \"+\",\".join(str(step) for step in self.step_record))\n",
    "                #self.file_manager.write(self.file_name_train, \"reward_record: \".join(self.reward_record))\n",
    "                self.file_manager.write(self.file_name_train, \"reward_record: \"+\",\".join(str(reward) for reward in self.reward_record))\n",
    "\n",
    "                #Periodically save the model.\n",
    "                if episode % 1000 == 0:\n",
    "                    self.model_saver.save_model(sess, episode)\n",
    "                    print(\"Model save_model\")\n",
    "\n",
    "                if len(self.reward_record) % 10 == 0:\n",
    "                    print(current_step, np.mean(self.reward_record[-10:]), e)\n",
    "                    \n",
    "                self.file_manager.write(self.file_name_train, horizontal_delimiter)\n",
    "            ##\n",
    "            self.model_saver.save_model(sess, self.num_episodes)\n",
    "        print(\"Percent of succesful episodes: \" + str(100*sum(self.reward_record)/self.num_episodes) + \"%\")\n",
    "\n",
    "    \"\"\" Auxiliary Methods \"\"\"\n",
    "    # Originally called updateTargetGraph\n",
    "    def set_target_graph_vars(self, tfVars, tau):\n",
    "        total_vars = len(tfVars)\n",
    "        op_holder = []\n",
    "\n",
    "        for idx,var in enumerate(tfVars[0:total_vars//2]): # Select the first half of the variables (mainQ net)\n",
    "            op_holder.append(tfVars[idx+total_vars//2].assign((var.value()*tau)+((1-tau)*tfVars[idx+total_vars//2].value())))\n",
    "\n",
    "        return op_holder\n",
    "\n",
    "    \n",
    "    # Originally called updateTarget\n",
    "    def update_target_graph(self, op_holder, sess):\n",
    "        for op in op_holder:\n",
    "            sess.run(op)\n",
    "         \n",
    "    def test(self):\n",
    "        step_record_t = []\n",
    "        reward_record_t = []\n",
    "        current_steps_t = 0\n",
    "        \n",
    "        load_model = True\n",
    "        num_test_episodes = 3000\n",
    "        \n",
    "        horizontal_delimiter = \"\"\n",
    "        for _ in range(20):\n",
    "            horizontal_delimiter += \"-\"\n",
    "        \n",
    "        \n",
    "        with tf.Session() as sess:\n",
    "            if load_model == True:\n",
    "                print('Loading Model...')\n",
    "                self.model_saver.restore_model(sess)\n",
    "                \n",
    "            for episode in range(num_test_episodes):\n",
    "                s = self.env.reset_for_testing()\n",
    "                d = False\n",
    "                episode_steps_t = 0\n",
    "                episode_reword_sum_t = 0\n",
    "                \n",
    "                self.file_manager.write(self.file_name_test, \"episode {0}\".format(episode+1))\n",
    "\n",
    "                #Q-Network\n",
    "                while episode_steps_t < self.max_ep_length: \n",
    "                    episode_steps_t+=1\n",
    "                    a = sess.run(self.mainQN.predict, feed_dict={self.mainQN.state:[s]})[0]\n",
    "                    s1, r, d = self.env.step_for_testing(a)\n",
    "                    \n",
    "                    current_steps_t += 1\n",
    "                    episode_reword_sum_t += r\n",
    "                    s = s1\n",
    "                    \n",
    "                    if d == True:\n",
    "                        break\n",
    "                        \n",
    "                step_record_t.append(episode_steps_t)\n",
    "                reward_record_t.append(episode_reword_sum_t)\n",
    "                \n",
    "                self.file_manager.write(self.file_name_test, \"step_record: \"+\",\".join(str(step) for step in step_record_t))\n",
    "                self.file_manager.write(self.file_name_test, \"reward_record: \"+\",\".join(str(record) for record in reward_record_t))\n",
    "\n",
    "                # print(episode_reword_sum_t)\n",
    "\n",
    "                if len(reward_record_t) % 10 == 0:\n",
    "                    accumR = np.mean(reward_record_t[-10:])\n",
    "                    log = str(current_steps_t) +\"\\t\"+ str(accumR) \n",
    "                    print(log)\n",
    "                    #f.write(log)\n",
    "                \n",
    "                self.file_manager.write(self.file_name_test, horizontal_delimiter)\n",
    "    \n",
    "        final_log = \"Percent of sucessful episodes: \" + str(sum(reward_record_t)/num_test_episodes) + \"%\"\n",
    "        print(final_log)\n",
    "        return reward_record_t\n",
    "        \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class DQN():\n",
    "    def __init__(self, net_name, state_size, action_size, hiddens, num_frames):\n",
    "        self.net_name = net_name\n",
    "\n",
    "        with tf.variable_scope(self.net_name):\n",
    "\n",
    "            #self.state = tf.placeholder(shape=[None, num_frames, state_size], dtype=tf.float32)\n",
    "            self.state = tf.placeholder(shape=[None, state_size], dtype=tf.float32)\n",
    "            #self.input_state = tf.reshape(self.state, [-1, num_frames * state_size])\n",
    "\n",
    "            # Weights of each layer\n",
    "            self.W = {\n",
    "                'W1': self.init_weight(\"W1\", [state_size, hiddens[0]]),\n",
    "                'W2': self.init_weight(\"W2\", [hiddens[0], hiddens[1]]),\n",
    "                'W3': self.init_weight(\"W3\", [hiddens[1], hiddens[2]]),\n",
    "                'AW': self.init_weight(\"AW\", [hiddens[2]//2, action_size]),\n",
    "               #'AW': self.init_weight(\"AW\", [hiddens[2]//2, hiddens[2]]),\n",
    "                'VM': self.init_weight(\"VM\", [hiddens[2]//2, 1])\n",
    "            }\n",
    "\n",
    "            self.b = {\n",
    "                'b1': self.init_bias(\"b1\", hiddens[0]),\n",
    "                'b2': self.init_bias(\"b2\", hiddens[1]),\n",
    "                'b3': self.init_bias(\"b3\", hiddens[2])\n",
    "            }\n",
    "\n",
    "            # Layers\n",
    "            self.hidden1 = tf.nn.relu(tf.add(tf.matmul(self.state, self.W['W1']), self.b['b1']))\n",
    "            self.hidden2 = tf.nn.relu(tf.add(tf.matmul(self.hidden1, self.W['W2']), self.b['b2']))\n",
    "            self.hidden3 = tf.nn.relu(tf.add(tf.matmul(self.hidden2, self.W['W3']), self.b['b3']))\n",
    "\n",
    "            # Compute the Advantage, Value, and total Q value\n",
    "            self.A, self.V = tf.split(self.hidden3, 2, 1)\n",
    "            self.Advantage = tf.matmul(self.A, self.W['AW'])\n",
    "            self.Value = tf.matmul(self.V, self.W['VM'])\n",
    "            self.Qout = self.Value + tf.subtract(self.Advantage, tf.reduce_mean(self.Advantage, axis=1, keep_dims=True))\n",
    "\n",
    "            # Calcultate the action with highest Q value\n",
    "            self.predict = tf.argmax(self.Qout, 1)\n",
    "\n",
    "            # Compute the loss (sum of squared differences)\n",
    "            self.targetQ = tf.placeholder(shape=[None], dtype=tf.float32)\n",
    "            self.actions = tf.placeholder(shape=[None], dtype=tf.int32)\n",
    "            self.actions_one_hot = tf.one_hot(self.actions, action_size, dtype=tf.float32)\n",
    "\n",
    "            self.Q = tf.reduce_sum(tf.multiply(self.Qout, self.actions_one_hot), axis=1)\n",
    "            self.td_error = tf.square(self.targetQ - self.Q)\n",
    "            self.loss = tf.reduce_mean(self.td_error)\n",
    "\n",
    "            self.trainer = tf.train.AdamOptimizer(learning_rate=0.0001)\n",
    "            self.updateModel = self.trainer.minimize(self.loss)\n",
    "\n",
    "    def init_weight(self, name, shape):\n",
    "        return tf.get_variable(name=name, shape=shape, initializer=initializer.xavier_initializer())\n",
    "                               #initializer=tf.contrib.layers.xavier_initializer())\n",
    "\n",
    "    def init_bias(self, name, shape):\n",
    "        return tf.Variable(tf.random_normal([shape]))\n",
    "        #return tf.get_variable(name=name, shape=shape, initializer=tf.constant(np.random.rand(range(shape))))\n",
    "                              # initializer= tf.random_initializer())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class ExperienceBuffer():\n",
    "    def __init__(self, buffer_size = 50000):\n",
    "        self.buffer = []\n",
    "        self.buffer_size = buffer_size\n",
    "\n",
    "    def add(self, experience):\n",
    "\n",
    "        if len(self.buffer) + len(experience) >= self.buffer_size:\n",
    "            self.buffer[0:(len(experience) + len(self.buffer)) - self.buffer_size] = []\n",
    "\n",
    "        self.buffer.extend(experience)\n",
    "\n",
    "    def sample(self, size):\n",
    "        return np.reshape(np.array(random.sample(self.buffer, size)),[size,5])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class ModelSaver():\n",
    "    def __init__(self, path):\n",
    "        self.saver = tf.train.Saver()\n",
    "        self.ckptPath = path\n",
    "        if not os.path.exists(path):\n",
    "            os.makedirs(path)\n",
    "\n",
    "    def restore_model(self, sess):\n",
    "        ckpt = tf.train.get_checkpoint_state(self.ckptPath)\n",
    "        self.saver.restore(sess, ckpt.model_checkpoint_path)\n",
    "\n",
    "    def save_model(self, sess, num_episode):\n",
    "        self.saver.save(sess, self.ckptPath+'/model-'+str(num_episode)+'.cptk')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class FileManager():\n",
    "    def __init__(self):\n",
    "        return\n",
    "\n",
    "    # If there is no file, create file with name firstly\n",
    "    # If there is file with name, append content in the file \n",
    "    def write(self, name=\"default_filename\", content=\"\"):\n",
    "        name = name+\".txt\"\n",
    "        with open(name, 'a') as f:\n",
    "            f.write(content)\n",
    "            f.write('\\n')\n",
    "        return\n",
    "    def read(self, name=\"default_filename\"):\n",
    "        name = name+\".txt\"\n",
    "        try:\n",
    "            with open(name, 'r') as f:\n",
    "                data = f.read()\n",
    "                return data\n",
    "        except FileNotFoundError as e:\n",
    "            print(e)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class DrawGraph():\n",
    "    def __init__(self, reward_record):\n",
    "        self.reward_record = reward_record\n",
    "  \n",
    "    def plot(self):\n",
    "        rMat = np.resize(np.array(self.reward_record), [len(self.reward_record)//100,100])\n",
    "        rMean = np.average(rMat, 1)\n",
    "        plt.plot(rMean)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
     ]
    }
   ],
   "source": [
    "def main():\n",
    "    #env = gameEnv(partial = False, size=15)\n",
    "\n",
    "    #agent = Agent([256, 256, 256], 4, 10, 6, env, 1e-3, 32, 10000, 100, 50000,\n",
    "    #              1, 0.01, 10000, 0.99, 0.001, 5, False, \"./log_dir\")\n",
    "\n",
    "    # The parameters of Agent class are as follows:\n",
    "    # hiddens, num_frames, state_size, action_size, env, lr, batch_size, num_episodes, pre_train_steps,\n",
    "    # max_ep_length, buffer_size, start_e, final_e, gamma, tau, update_freq, load_model, logs_dir\n",
    "   \n",
    "    \n",
    "    \n",
    "    name=\"test\"\n",
    "    log = LogManager()\n",
    "    log.write(name, \"13432432\")\n",
    "    print(log.read(name))\n",
    "    \n",
    "    \"\"\"\n",
    "    agent = Agent([256, 256, 256], 4, 10, 6, env, 1e-3, 32, 10000, 10000, 500, 50000, 1, 0.01, 0.99, 0.001, 5, False, \"./logs_dir\")\n",
    "    #agent.learn()\n",
    "    \n",
    "    reward_record_t = agent.test()\n",
    "    graph = DrawGraph(reward_record_t)\n",
    "    graph.plot()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading Model...\n",
      "INFO:tensorflow:Restoring parameters from ./logs_dir/model-10000.cptk\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[2017-06-13 02:23:22,221] Restoring parameters from ./logs_dir/model-10000.cptk\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "80\t3.5\n",
      "183\t4.0\n",
      "289\t4.0\n",
      "375\t4.0\n",
      "456\t4.0\n",
      "583\t4.0\n",
      "696\t4.0\n",
      "807\t4.0\n",
      "937\t4.0\n",
      "1057\t4.0\n",
      "1129\t4.0\n",
      "1235\t4.0\n",
      "1294\t4.0\n",
      "1384\t3.5\n",
      "1520\t4.0\n",
      "1654\t4.0\n",
      "1792\t4.0\n",
      "1875\t4.0\n",
      "1984\t4.0\n",
      "2069\t4.0\n",
      "2165\t4.0\n",
      "2242\t4.0\n",
      "2375\t4.0\n",
      "2454\t4.0\n",
      "2532\t4.0\n",
      "2631\t4.0\n",
      "2744\t4.0\n",
      "2812\t4.0\n",
      "2898\t4.0\n",
      "3017\t4.0\n",
      "3114\t4.0\n",
      "3192\t4.0\n",
      "3294\t4.0\n",
      "3394\t4.0\n",
      "3497\t4.0\n",
      "3601\t4.0\n",
      "3702\t4.0\n",
      "3766\t4.0\n",
      "3841\t4.0\n",
      "3932\t4.0\n",
      "4027\t4.0\n",
      "4103\t4.0\n",
      "4209\t4.0\n",
      "4318\t4.0\n",
      "4414\t4.0\n",
      "4496\t4.0\n",
      "4625\t4.0\n",
      "4740\t4.0\n",
      "4864\t4.0\n",
      "4974\t4.0\n",
      "5072\t4.0\n",
      "5146\t4.0\n",
      "5246\t4.0\n",
      "5325\t4.0\n",
      "5422\t4.0\n",
      "5511\t4.0\n",
      "5617\t4.0\n",
      "5732\t4.0\n",
      "5833\t4.0\n",
      "5931\t4.0\n",
      "6068\t4.0\n",
      "6179\t4.0\n",
      "6257\t4.0\n",
      "6334\t4.0\n",
      "6449\t4.0\n",
      "6504\t4.0\n",
      "6609\t4.0\n",
      "6681\t4.0\n",
      "6779\t4.0\n",
      "6877\t4.0\n",
      "6943\t3.5\n",
      "7049\t4.0\n",
      "7155\t4.0\n",
      "7262\t4.0\n",
      "7350\t4.0\n",
      "7446\t4.0\n",
      "7547\t4.0\n",
      "7648\t4.0\n",
      "7757\t4.0\n",
      "7829\t4.0\n",
      "7936\t4.0\n",
      "8016\t4.0\n",
      "8119\t3.5\n",
      "8206\t4.0\n",
      "8282\t4.0\n",
      "8374\t3.5\n",
      "8460\t4.0\n",
      "8543\t4.0\n",
      "8653\t4.0\n",
      "8749\t4.0\n",
      "8838\t4.0\n",
      "8938\t4.0\n",
      "9048\t4.0\n",
      "9141\t4.0\n",
      "9229\t4.0\n",
      "9336\t4.0\n",
      "9465\t3.5\n",
      "9583\t4.0\n",
      "9698\t4.0\n",
      "9768\t4.0\n",
      "9864\t4.0\n",
      "9957\t4.0\n",
      "10043\t4.0\n",
      "10134\t4.0\n",
      "10223\t4.0\n",
      "10319\t4.0\n",
      "10401\t3.5\n",
      "10477\t4.0\n",
      "10569\t4.0\n",
      "10677\t4.0\n",
      "10780\t4.0\n",
      "10862\t4.0\n",
      "10941\t4.0\n",
      "11034\t4.0\n",
      "11123\t4.0\n",
      "11210\t4.0\n",
      "11296\t4.0\n",
      "11400\t4.0\n",
      "11479\t4.0\n",
      "11600\t4.0\n",
      "11710\t4.0\n",
      "11815\t4.0\n",
      "11932\t4.0\n",
      "12017\t4.0\n",
      "12083\t4.0\n",
      "12182\t4.0\n",
      "12301\t4.0\n",
      "12399\t4.0\n",
      "12510\t4.0\n",
      "12600\t4.0\n",
      "12700\t4.0\n",
      "12835\t4.0\n",
      "12975\t4.0\n",
      "13048\t4.0\n",
      "13141\t4.0\n",
      "13221\t3.5\n",
      "13316\t4.0\n",
      "13428\t3.5\n",
      "13493\t4.0\n",
      "13570\t4.0\n",
      "13666\t4.0\n",
      "13767\t4.0\n",
      "13847\t4.0\n",
      "13964\t4.0\n",
      "14101\t4.0\n",
      "14198\t4.0\n",
      "14304\t4.0\n",
      "14413\t4.0\n",
      "14505\t4.0\n",
      "14595\t4.0\n",
      "14735\t4.0\n",
      "14856\t4.0\n",
      "14939\t4.0\n",
      "15045\t4.0\n",
      "15153\t4.0\n",
      "15250\t3.5\n",
      "15339\t4.0\n",
      "15443\t4.0\n",
      "15528\t4.0\n",
      "15622\t4.0\n",
      "15705\t4.0\n",
      "15805\t3.0\n",
      "15897\t4.0\n",
      "15999\t4.0\n",
      "16117\t4.0\n",
      "16200\t4.0\n",
      "16303\t4.0\n",
      "16417\t4.0\n",
      "16526\t4.0\n",
      "16645\t4.0\n",
      "16724\t3.5\n",
      "16837\t4.0\n",
      "16931\t4.0\n",
      "17022\t4.0\n",
      "17106\t4.0\n",
      "17214\t4.0\n",
      "17299\t3.5\n",
      "17393\t4.0\n",
      "17492\t4.0\n",
      "17595\t3.5\n",
      "17684\t4.0\n",
      "17775\t4.0\n",
      "17873\t4.0\n",
      "17955\t4.0\n",
      "18035\t3.5\n",
      "18138\t3.5\n",
      "18219\t4.0\n",
      "18296\t4.0\n",
      "18380\t4.0\n",
      "18472\t4.0\n",
      "18537\t3.5\n",
      "18603\t4.0\n",
      "18706\t4.0\n",
      "18812\t4.0\n",
      "18895\t4.0\n",
      "18961\t4.0\n",
      "19052\t4.0\n",
      "19170\t4.0\n",
      "19268\t4.0\n",
      "19359\t4.0\n",
      "19468\t4.0\n",
      "19564\t4.0\n",
      "19677\t4.0\n",
      "19804\t4.0\n",
      "19916\t4.0\n",
      "20001\t4.0\n",
      "20071\t4.0\n",
      "20156\t4.0\n",
      "20259\t4.0\n",
      "20368\t4.0\n",
      "20457\t4.0\n",
      "20563\t4.0\n",
      "20659\t4.0\n",
      "20768\t4.0\n",
      "20856\t4.0\n",
      "20952\t4.0\n",
      "21027\t3.5\n",
      "21157\t4.0\n",
      "21250\t3.5\n",
      "21370\t4.0\n",
      "21463\t4.0\n",
      "21555\t4.0\n",
      "21655\t4.0\n",
      "21728\t4.0\n",
      "21825\t4.0\n",
      "21910\t3.5\n",
      "22026\t4.0\n",
      "22129\t4.0\n",
      "22221\t4.0\n",
      "22303\t4.0\n",
      "22397\t4.0\n",
      "22494\t4.0\n",
      "22596\t4.0\n",
      "22679\t4.0\n",
      "22807\t4.0\n",
      "22924\t4.0\n",
      "23021\t4.0\n",
      "23089\t4.0\n",
      "23187\t3.5\n",
      "23262\t3.0\n",
      "23350\t3.5\n",
      "23472\t4.0\n",
      "23575\t4.0\n",
      "23691\t4.0\n",
      "23772\t4.0\n",
      "23843\t4.0\n",
      "23950\t4.0\n",
      "24043\t4.0\n",
      "24124\t4.0\n",
      "24229\t4.0\n",
      "24322\t4.0\n",
      "24391\t4.0\n",
      "24472\t4.0\n",
      "24590\t4.0\n",
      "24690\t4.0\n",
      "24778\t4.0\n",
      "24882\t4.0\n",
      "24981\t4.0\n",
      "25093\t4.0\n",
      "25176\t4.0\n",
      "25277\t4.0\n",
      "25355\t4.0\n",
      "25458\t4.0\n",
      "25522\t4.0\n",
      "25615\t4.0\n",
      "25700\t4.0\n",
      "25804\t4.0\n",
      "25896\t4.0\n",
      "25980\t4.0\n",
      "26068\t4.0\n",
      "26165\t4.0\n",
      "26269\t3.5\n",
      "26370\t4.0\n",
      "26470\t4.0\n",
      "26549\t4.0\n",
      "26641\t4.0\n",
      "26725\t3.5\n",
      "26813\t3.5\n",
      "26927\t4.0\n",
      "27032\t4.0\n",
      "27126\t4.0\n",
      "27221\t3.5\n",
      "27328\t4.0\n",
      "27415\t4.0\n",
      "27497\t4.0\n",
      "27613\t4.0\n",
      "27701\t3.5\n",
      "27832\t3.5\n",
      "27928\t4.0\n",
      "28036\t4.0\n",
      "28125\t4.0\n",
      "28219\t4.0\n",
      "28309\t4.0\n",
      "28403\t4.0\n",
      "28535\t4.0\n",
      "28655\t4.0\n",
      "28744\t4.0\n",
      "28847\t4.0\n",
      "28935\t4.0\n",
      "29021\t4.0\n",
      "Percent of sucessful episodes: 3.9483333333333333%\n"
     ]
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAX0AAAD7CAYAAACG50QgAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz\nAAALEgAACxIB0t1+/AAAIABJREFUeJztnXuQXFl93z+/0TAjjd7PkbSsViskJI20sw9icNmQmiWy\nvRBsnKVISJwypFLBcVIJCYSKcRG0IqawXS47FIbgsiG7Jg+w2cryWgowy0CFgmXR7mpGGj29ktFj\nZ7QSesysHqPRnPxx+u72tnq67+Pce+65/ftUTanVffv2uX27P33u97zEGIOiKIrSGXT5LoCiKIpS\nHCp9RVGUDkKlryiK0kGo9BVFUToIlb6iKEoHodJXFEXpILp9FwBARLTfqKIoSgqMMZJk+9LU9I0x\nlf3bvXu39zLo8enxdeLxVfnYjElXVy6N9BVFUZT8UekriqJ0ECr9AhgaGvJdhFzR4wubKh9flY8t\nLZI2F3JaCBFThnIoiqKEhIhgQm3IVRRFUfJHpa8oitJBqPQVRVE6CJW+oihKB6HSVxRF6SBU+oqi\nKB2ESl9RFKWDUOkriqJ0ECp9RVGUDkKlryiK0kGo9BVFUToIlb6iKEoHodJXFEXpIFT6iqIoHURs\n6YtIl4g8LSJfafJYj4h8QUSOisgPRWRD3WMfqt1/UER+2VXBFUVRlOQkqem/Dxib47F/CfzMGLMF\n+G/AHwKIyADwj4HtwFuAT4tIormfFUVRFHfEkr6IvBp4K/AXc2zyduCR2u0vAW+u3f414AvGmBlj\nzAngKPD61KVVFEVRMtEdc7s/AT4ILJ3j8duAkwDGmJsicklEVtTu/2Hddqdr9xXClStw8WJRr+aG\nhQth6VzvcuAYAxMTMDsbb/vFi+2fa6anoafH/X5DpOrvxc9+Bteuxdu2pwdWrcq3PO2YmoKbN/N1\nQFvpi8g/BCaMMc+KyBDQLJ5pdp9pcf8tPPTQQy/dHhoacrK25a/+KoyMwKtelXlXhWCMPeETE1DF\nEOzrX4d3vhOWL2+/7c2bsGQJHD3qvhxvehN85jNw773u9x0ad90F3/gGbNrkuyTuOX8e1q+HlSvj\nbf/CC3DyJKxdm2+5WvHIIzA2Bp/6VPPHh4eHGR4ezvQacWr6vwj8moi8FVgALBaRvzTG/GbdNieB\n24EzIjIPWGqMuSAip2r3R7waONPsReql7wJjYO9eK43Vq53uOlfWrYNTp+D229tvGxp798L73w8f\n+1j7ba9etT8Oxrj/ATx+HH7yE5X+xYtw5Aj89KfVlP6+ffCGN8D3vx9v+507rfh9Sv/ixda1/MYK\n8Z49exK/RttM3xjzu8aYDcaYTcC7gCcahA/wVeDdtdvvBJ6o3f4K8K5a7547gc3AjxOXMgUnT0Jf\nX1jCBxgctFcnVWRkxB5fHBYssJfbly+7LcPMDJw7V933OAmjo/bfiQm/5ciLJJ83gGXL/MfBly7Z\ncuRJ6n76IrJHRN5W++9ngVUichT4D8DvABhjxoC/wvb6eRz4N8aYpvGOa5Ke8LKg0n+Z/n73Qjp3\nzl49VPU9TkL0Hqj0LUuXWun6pF1N3wVxG3IBMMZ8D/he7fbuuvuvY7tmNnvOx4GPZyhjKvbtC1f6\nX/+671K4Z2oKTp+GLVviPyeS/mtf664cExP28n1kJJ/oKCRGRux7UVXp79sH731v/O3LIP1Ll/KX\nfmVH5GpNv1zs3w/bt0N3gmpGfz+Mj7stx8QE7NgB8+fbCLCT2bcPdu1y/x6XgZkZOHjQ5vRx0Xgn\ncEZG4O67fZciOdu324bGuN3MQiHN+cgj3hkft/ut6o9rXGZn7Q/xrl3VrOkfPQq33QaLFsV/Thlq\n+kXEO5WU/tWrcOIEbN3quyTJ6emxEcjYXGOfAyXNlVce0p+YUOkDPPec7cq4dWs1pZ/m87ZsmX/p\na7yTkrExmwOHOuikikJS6ZeL6Hzk8R6XgTSft6VLNd4JllDz/IiqCSnqLXPXXcmel5f0166t3nuc\nlHrpnz1rz1GVSCt93zV9jXdSotIvF2nHTORZ069q20lcou9IX5+9IvYtO9eEGO/cuGH/+vryfR2V\nfgkZHLQ9K6pS+0p7PvKUflXbTuJS37BetYjn4kU7586ddyZ7nu94J8rz8+5GXDnpGxNuH/2Idete\nnpysCmSVvssfv0j6UL0rqrhMTcHzz8Pmzfb/VZP+6KjtqtmV0G6+450ioh2ooPTHx+0vpc/5M7Ii\nUi0hpZX+okX2vZiaclOOmzftJFxRzFSl9zgJjWMmqib9tJ833/30i2jEhQpKPzrhoY+0rJKQssRt\nLoV0/rytSUWzrlbpPU5C4/lQ6Vt81/SL6K4JFZZ+6FRFSNeu2QbTbdvSPd+lkOqjHahe20lcVPrN\nWbgQrl+3jak+0HgnJSr9cjE2ZhtM046ZyFP6VWs7iUuVpR+NNE7aPRhsOuCztq/xTkpCb8SN2LHD\nznU+Pe27JNnIej7ylH7V2k7i0GzMRJWk/9xzsGJFenn6lL7W9FMwPW3n3Nixw3dJsrNgAdxxBxw+\n7Lsk2cg6B1Ke0ofOk36zMRNVkn7WK33fNX2VfkIOHbJ9c+fP910SN1RBSFm/hC6n/p1L+vv2udl/\nCDS78qqa9LNUMnz24NF4JwVVyfMjQpe+izETWtN3S7PvSB7jIXwRck1f450UqPTLhYsxE3lLf2Cg\nGm0ncWn2HXE9HsInWR3gcyoGjXdSoNIvFy7GTOQt/b6+arSdxGWu70gVIp7GkcZp8DkVg8Y7Kaia\n9DdssB/kc+d8lyQdLs5H3tIHmwGH/OMal2idiWZjJqog/TSrszWi8U5AvPACXLkCt9/uuyTuiLoU\njo76Lkk6XEh/8WK79N2VK9n2MztrPyNr1tz6WOhXVHFptc5EFaTv4vOm8U5AjI5WY/qFRkIWkosv\noYgbIV24YEdc9vbe+ljI73ESWp0Plb5F452AqFq0ExGqkKanbQPpwED2fbkQ0lzRDoT7HidFpd8e\nX/GOMRrvJEalXy4OH4aNG+0gs6z099ueQFloJf3Q207iUmXpp12drRFf/fSvXrUTARaxxKtKv+Ts\n3Gmz2Js3fZckGS7PR941/dDbTuLQbsxE6NJPuzpbI75q+kXV8qEi0p+ZsWLcudN3SdyzeLHt5370\nqO+SJMPlHEh5Sx/CvaKKS7sxEy6upnzi6vPmS/pFNeJCRaR/9CisX28HmVSREIUUUk0fwnyPk9Bu\nzEToNX1Xnzdf8U5RjbhQEelnnW+j7ITYj9zlOVHpZ6edFKsgfReft6imX/SUFBrvJKSqeX5EaEJy\nPWaiCOlHbSczM9lep6y0+44sWWKP/cUXiyuTS1w5oLfXrq177Vr2fSVB452EqPTLhesxEy6kPz7e\neg6gqO3k2LFsr1NW2mXeUd4fYm0/Gmm8daub/fmIeDTeSUjVpb9pk+1O6HP9ziS4Ph9F1PQhvB/X\nuETrTLQbMxFqxNNqpHEafDTmaryTgIsX4Wc/s/PoV5WuLhs/hNKl0LX0ly2zl9tpL7mNgbNn20s/\nxLaTOBw6FG/MRKjSz+PzVrT0Nd5JwOioFWJX8EfSmpBqoa6/hCJ2zpy0Qrp40S6s025xnZDe4yTE\nPR8qfYuPqRg03klA1aOdiFCElNeYiSxCihPtQDjvcVLi9mxR6Vs03ik5Kv1ycexYPmMmipB+aG0n\ncalyTd/F6myN+GrIVenHpFOkf9ddNsqanfVdktbkdT6KkH5obSdxqbL0XazO1oiPmr7GOzGZnbUL\nJ2SdZCkEli+3fydO+C5Ja0KWPoRzRRWXJGMmQpS+i9XZGtF4p8Q89xysWFHcL6RvQhCS60vtCJV+\nOpKMmQhR+nl83rSffonplGgnYnDQfsjLjNb0y0WS8xGi9PP4vGlNv8R0ovTLLKRozMSmTe73XZT0\nQ2k7iUuS70jW8RA+qIL0Z2fteg6LFxfzesFLv8oTrTVS9sFDeY6ZKEr6obSdxCWJFLOOhyiaaKTx\njh1u91t0vDM5aXu7zZtXzOsFL/1Oqulv2QKnT9taQRnJ83wUJX0o/xVVXNKMmQgp4jl0yI7Ebzfo\nLilF1/SLjHYgYOlPTcHzz8Pmzb5LUhzd3bB9Oxw44LskzclT+itW2BrR9HSy5xmTTvplbzuJQ5ox\nEyFJP6/PW9HSL7KPPgQs/f37rQC7u32XpFjKXAvNU/pdXTZ6OHs22fMmJ+1l88KF8Z9T9hgtLml6\ntqj0i493iuy5AwFLv9OinYiySr+IMRNphJS0lg/lfY+TkuY7otK3awtMThbXmK/xTkxU+uXi+PH8\nx0ykWcc1jfTL3nYSlzQdHVT69sqwr8+Kvwg03olJp0u/6OXc2lHE+Siqpl/2tpO4VLmm73p1tkaK\nnF5Z450YGGM/0J0w/UIjq1fb3gqnTvkuySupkvShvFdUcUm7zkQo0ne9OlsjRTbmarwTg5Mn7eXX\n6tW+S+KHMgpJpV8u0o6ZCEX6eX/eimzM1XgnBnnN7xIKZRRSEedEpR+ftFJM027ig7w/b0XW9DXe\niUGn5vkRZetHPjUFZ87YBtA88SH9srWdxCXtd2TFCns+r193XyaX5O0AjXdKhkq/XLXQosZMFCn9\nsradxCXtd6Sryx570vEQRTIzAwcPul+drZ6i4x2t6beh0+bcaWT7dttFsiwTYxV1PoqUPpTvxzUu\nWcdMlD3XP3o0n9XZ6tGafom4etVOhrV1q++S+KOnx0YpY2O+S2Ip6spr5Ur7RbxxI/5zJibSr6oU\nqvSzjplYu7bc0i+iklF0pl8q6YtIr4g8KSLPiMioiOxuss0GEfkbEdknIk+IyPq6x26KyNO15z+W\ntcBjY/Da11rxdTJlElJR0p83z4r/hRfibf/ii7bWm7ZGWLa2k7hkPR9lr+kX8Xnr6HjHGHMduN8Y\ncy9wD/AWEXl9w2Z/BDxsjLkb+Cjw+3WPvWiMuc8Yc68x5tezFrjT8/yIski/6DETSYQURTtp+3KH\nOgePSj87HR/vGGOu1G72At1AY5+GAeCJ2rbDwNvrHnM6fEKlbymL9IseM5FG+mnZtq1cbSdxUeln\npyjp37hh//r68n+tiFj9LUSkC9gLvAb4lDHmqYZNngXeAXxSRB4EFonIcmPMBaBXRH4MzAB/YIz5\ncrPXOHMmXoH37oUHHoi3bZWJogdj3I9KvH4dzp+Pt+33vlfsj3ASIY2PZ5N+fdvJffel30/R7NsH\nv/d76Z/f3w9PNX7DEzIzYz+XrhcGSTvSOClFxTtRnp/XyOJmxJK+MWYWuFdElgCPiciAMaa+GfGD\nwJ+KyHuA7wOnsZIH2GCMGReRO4EnRGTEGHO88TW2bn3opds9PUP09g41LUtvb1hfwLxYt84Kf3zc\n3nbJb/82PPZY/MUpPvABt6/fiiJr+vDyFVUon7lonYksYyZc1PQ//GF79ef6s3HokL0Cy2N1tnqK\nquknjXaGh4cZHh7O9JqJelYbYy6LyDDwADBWd//z2Jo+IrIQeIcxZrL22Hjt3+O1594L3CL9ycmH\nUh1ApyJihTQ66l76e/fCt78Nr3ud2/26IMmIUZfSDwUXYyZcSP+pp9wvYwj2B239+vbbZWXp0mJr\n+nEZGhpiaGjopf/v2bMn8WvG6b2zSkSW1m4vAHYBhxq2WSny0gXKh4DP1e5fJiI90X6AX6Dux0LJ\nRh5Cmp6GI0dgYMDtfl3hq6YfCi7y7qzSN8ZGTHm0C7g4p3EoapbNonvuQLyG3HXAd0XkWeBJ4JvG\nmMdFZI+IvK22zRBwWEQOAWuAj9Xu3w78RESeAb4DfNwYcwjFCXkI6fBh2LgRFixwu19X+JB+1HYS\nAi56UqUZD1HP+LhtEwpZ+gsX2rattO9BXIruuQMx4h1jzChwS6JpjNldd/tR4NEm2/wQ0L42OTE4\nCJ/4hNt9ln0yu6KlH7WdZBnkVSQjI/Dgg9n2UT8eIk2UMjKS3wCviYlirkJFXs71V63K73WKHpgF\nAY7IVV5mYMBGMUkXC29F2bvEFi39qO0khIjH5ZiJLBHPvn2wa1fYNX0opjG3rPGOUlL6+uCOO2wk\n44qyz2u0erXtsnfzZvttXQkiFOm7HDORRfojIzA0ZJcbdD1bZ9Wk7yPeUekHjutRo2Wv6Xd325rR\nuXOtt7t61V4BufhChSJ9l+cuq/TvuSef2TqLlH4RffW1pq8kxqWQ8l531BVxhDQxAWvWuBn0otKP\nz/S0nQVzYCCfkb1a08+OSj9wXAop73VHXRFX+q7ksGOH+7aTPCiD9A8dern3l+vGXJdXb3EoKtNX\n6SuJcCn9skc7EXEGaLmU/oIF7ttO8qAM0q9vE3Jd03d59RYHjXeUUrJhgx163y7jjkNI0i+ypg/l\nj3iuXbPrTGzb5mZ/WaQffYbykH5R0Q5ovKOUlPrpGLKi0p+bskt/bMzOt+NqnQmVfjFTMWi8o6TC\nhZBmZqw48lx31BVxsuJOk77rQXUq/WKmYtB4R0mFCyEdO5b/uqOu0Jr+rbi+SksyHiKisfdX6NLP\nO94xRuMdJSUuhBRKtAN+pO+y7SQPXJ+/aDxE3KUp4dbeX1WQfp7xztWr9n0ueulXlX4F2LnTRjNJ\namWNqPRb47LtxDXRrJauz19SaTd+hkKXft7xjo9oB1T6lWDxYptzHz2afh9ln2itnjVrbI17dnbu\nbfIQRFkjnvFx+6PkekK4rNLPOltnI1lXQktK3vGOj2gHVPqVIauQyj7nTj2vehUsWTL3ko7Xr8OL\nL8Ly5W5ft6zSj2Truv96VunXz9bpAh81/TzjHR89d0ClXxmyzMFT1LqjLmklpLNnbUOk6yX1yi59\n1ySR/ly9v1xGPL4acvNaS0HjHSUTWYQ0Omq/rHmvO+qSVjLJa+77qO1kZqb9tkWSl/STTKNw9Gjz\n3l+upH/tmu0Z5PrqrRW9vfY7ce1aPvvXeEfJRBbph9SIG9FO+nnUCBcvtouqHDvmft9ZKENNf64y\nuJL+2bO2LafoikmeEY/W9JVMbNpkGzfTNDyp9ONTtoinflZL1ySVfrM2IVfSLzraicizMVdr+kom\nurps/JCmS6FKPz5lk36eaxqXqabvU/p51vRV+kom0ghpdhb273ezxF6RqPQtef5gq/Tz7auv8Y6S\nmTRCOn4cVqzw8+HLgkrfkqf044yHgNa9v0KXvsY7SqlJI6QQox1oLZM8B/FkaTvJgzwH1bUbDxHR\nqvdXFaSv8Y5SWu66y34B29XM6qmi9PMURJa2kzzI+/zFkXarMoQufY13lFKzfLn9O3Ei/nNClf6a\nNbYbX7OBM3kLoiwRTxFrGmeVfprZOpuh8Y47VPoVY3DQXvLHJaQ5d+qZPx/6+uDChVfef+MGXL5s\nh//nRVmkX8SaxlmlH83WmXV2Uo133KHSrxhJhDQ1BWfO2BWXQqSZkM6ehVWr8h3EUxbpF3GV1k76\ncXp/uYh4NN5xh0q/YiQR0v79dlBPd3e+ZcqLZjIpQg5p2k7yoAzSj9P7K6v0i7h6m4u84p3ZWZic\ntKO8i0alXzGSTLwWap4f0d9ve+rUU4T0o7aT48fzfZ12lEH6ccqQVfpFXL3NRV7TMExOwsKFdibS\nolHpV4wtW+D0aRvdtKMK0vdR04dss5q6YGYGDh7Mf03jONJvNyX32rW3/jgnwVe0A/nV9H1FO6DS\nrxzd3bB9Oxw40H5blX56fOf6Ra1p3E76cToCZK3p+5Z+HjV9Xz13QKVfSeIIyRi7TWjTL9TTydIv\n6ge7DPGOT+nn1ZCrNX3FKXGEdPKk7fK4enUxZcoDlX7+r9NqPMTUFDz/PGze3HofIUt/yRKbv7tu\ntNeavuKUOEIKPdoBv9JP0naSB0Wdv7nGQ4Dt/bV9e/veXyFLf948e/yTk27366uPPqj0K0kk/VbL\nvKn0s5Gk7SQPijx/c0k7bhlClj7kE/FovKM4ZfVqW0M7dWrubaok/foftyIF4SviuXTJToJW1JrG\nzbrGQvzPUNzZOufCt/Tz6MGj8Y7inHZCqoL0+/qgp+flL+TMjI0hVq0q5vV9SX9kpNg1jbPW9OPO\n1jkXZZC+6x48Gu8ozmklpGvX7MCibduKLVMe1C/efe6cHTRV1Ahjn9Iv8ge7mfST9v7KEvHkOVV2\nHDTeUYKg1cRrBw7YhsienmLLlAf1Mim6Rhin7SQPyiD9pL2/0kq/6Ku3Zmi8owRBq1poFaKdCJ/S\nj9pOTp4s7jWhHNJPWoa00n/hBTu3j8/5oTTeUYJg+3Yb4Vy7dutjcYbOh0Kj9NeuLfb1i454fKxp\nXB+hRRQlfd95Pmi8owRCT4+NcMbGbn1Ma/ruKHoOHh9rGs9V009ScQhZ+hrvKMHQrBZqTLgLpzTD\nt/SLrun7uErzGe+URfoa7yhB0ExI4+N2paWiY5C86ETpF/2D3d//yqkYrl2zS3Ju3ZpsH6FKX+Md\nJRiaCSmSRp5L7BWJb+lv2zZ320ke+JD+ggWvHA8xNpa891fI0ndd079xA6anbe8nH6j0K0zUbbO+\nS2GV8nzwL/1WbSd54Ov81b/PaeLBLNL3fVXquqYfRTu+Kl4q/Qqzbp0Vfv2XrarSj47TR62wqIgn\nWtO43ayWeVAv7TSfoVazdbaiLDV9l9L32YgLKv1KI3KrkKom/UWL7HFevmyH+fuYKroo6R84EG9W\nyzzIKv1Ws3W2oizSdxnv+GzEBZV+5akX0vQ0HDliF0OvEv39dunAJUvsPC9FU5T0ff5g119Rpe39\nlSbiKYP084h3fDXigkq/8tQL6fBh2LjRNsxVif5+KyJfcmjWdpIHPrvaRsLO0vsrqfRv3vR39VbP\nwoVw/bptgHWBxjtKrtTPwVOl/vn1+JZ+s7aTPChDTT9L76+k0j9/3srRx9VbPSL2KtJVbV/jHSVX\nduywkc70dPXy/Ij+fntsvqQftZ3MNcGdC3yvadwo/Sz7iEsZop0IlxGPxjtKrixYYCOdw4erNedO\nPf39MDrqVxB55/q+1zTudOm77MGj8Y6SO5GQqlzTv3zZryDynoPH9w92vfTTlqPZxG2tKJv0XfXg\nKX28IyK9IvKkiDwjIqMisrvJNhtE5G9EZJ+IPCEi6+see7eIHBGRwyLym64PQGnP4CB85ztw5Qrc\nfrvv0rgnEkOVa/q+f7D7++H55+Ho0fS9v0Ku6XdUvGOMuQ7cb4y5F7gHeIuIvL5hsz8CHjbG3A18\nFPh9ABFZDnwE+DngDcBuEfH4G9eZDA7Co49Wa/qFesog/YGBl9tO8sC39BctgnnzbFQ4f366fYQs\nfZc1/SDiHWPMldrNXqAbaOycNgA8Udt2GHh77f5fAb5ljLlkjLkIfAt4IGOZlYQMDtr4o4rRDpRD\n+vVtJ3ngW/pg398sZQhd+h3Ve0dEukTkGWAc+LYx5qmGTZ4F3lHb9kFgUa2WfxtQv67Q6dp9SoFs\n2GC7nPmWRl6UQfqQX8STZlbLPHAl/bjjGcokfZfxzsWLfuOdWAO6jTGzwL0isgR4TEQGjDH1U0x9\nEPhTEXkP8H2s3GeAZmFC01P+0EMPvXR7aGiIoaGhOEVTYiACDz4Ib3yj75Lkw5IlsGuXf0FE3TZ/\n4zfc7ndszM6343tN4ze9CbJ8Letn64wjvTJJf+lSOHXKzb6y1PSHh4cZHh7O9PpiEg4jFJGPAFPG\nmD+e4/GFwEFjzAYReRcwZIz517XHPgN81xjzxYbnmKTlUJSy8dWvwqc/Dd/4htv9PvywbYj//Ofd\n7tcHW7bA174W76rlttvgRz8qR+eDz34WfvAD+Nznsu9r9Wr7Q+6i+62IYIxJ1FIXp/fOqqjxVUQW\nALuAQw3brBR5qYnwQ0D01nwT+CURWVqLe36pdp+iVI684p0y5PmuiJvrz87aRdHXrMm/THFwlekb\nE0ZD7jrguyLyLPAk8E1jzOMiskdE3lbbZgg4LCKHgDXAxwCMMReA/wr8pPbcPbUGXUWpHBs22OmP\nz51zu99OlP6FC3bOm97e/MsUB1e9d65etbOk+ozq2mb6xphR4L4m9++uu/0o8Ogcz38YeDh1CRUl\nEOqnsn7zm93ss8prGrdifLw8eT64a8j13UcfdESuojjFdcQzPm7/9b16lCviSr9MjbjgLt7xHe2A\nSl9RnOJa+lVe07gVZZS+i3jHdx99UOkrilNcz8Hje84d14Qs/UuXsq+ZoPGOolSMnTttd7yZGTf7\nq1IjLoQr/fnzoavLNsRmQeMdRakYixbB+vVw7Jib/an0y4OLXF/jHUWpIK5y/enpbLNalpG4UzGU\nUfouevBovKMoFcSV9KM1jdPOallGFi2yjdJTU623K6P0XTTmaryjKBXElfSrFu1ExIl4yip9jXcU\nRbkFlX5r2knfGDh7tnzSdxHv+J5hE1T6iuKcTZvsVAxZBdGp0r940UZaCxYUV6Y4uIh3tKavKBWk\nq8t23RwdzbafTpV+GaMdcNeQq9JXlAqSNeI5dw5efLEc0wq7JlTpu8j0Nd5RlIoSLaiSlqpNv1BP\nyNLXeEdRlKZkrelXNdqBcKWv/fQVRZmTwUHYv98uBpKGqs25U0+o0s9a05+dhclJWLzYXZnSoNJX\nlBxYtgxWrIDjx9M9v8o1/bVrw5V+lpr+5KRdGGbePHdlSoNKX1FyIm3EMzMDBw/Cjh3uy1QGQq3p\nZ413yhDtgEpfUXIjrfSPHbOTti1a5L5MZWDxYvvDduVK88fLKv2s8U4ZpmAAlb6i5EZa6Vc52gHb\nI6lVbb/M0s9a01fpK0qFUenPzVzSN6a80l+yxObyaRvnNd5RlIqzZQucPt1+RslGOln6k5O2oXPh\nwuLL1I7ubujrs2VMg8Y7ilJxurth+3Y4cCDZ8zpF+tGi7/WUtZYfkSXi0XhHUTqApBHPpUtw/jzc\neWd+ZSoDc9X0yy79LD14yjAFA6j0FSVXkkp/dNRO1tZV8W9mqNLP0oNHa/qK0gEknYNn377qRzsw\nt/THx8st/Sw1fZW+onQAUU2/3ZqwEZ2Q50PYNX2NdxRFmZPVq+1iICdPxttepV9+6Wu8oyhKS+6+\nO16uPztrJ2lT6RdfnrhkjXe0pq8oHUDcxtzjx2HlynLUBvNm2TK4ds3+1VN26Wep6Ws/fUXpEOJK\nv1OiHbDy1/2zAAAIn0lEQVRTMaxZc2ttPwTpa0OuoigtUek3p1nEU3bpa7yjKEpbtm2z0U1jlNFI\np0t/asq2a/heZKQVaeOdGzdgetpO4+Ablb6i5ExPj52HZ2ys9XadLv2oll/mdYHTxjuXLtkJ28pw\nbCp9RSmAdhHP1BScOQObNxdXJt/MJf0ykzbeKUu0Ayp9RSmEdtI/cMBOztbdXVyZfBOi9NPGO2Xp\nuQMqfUUphHbS77RoB8KVftqavkpfUTqIaA6euaZjUOnb22vX+itPHBYtguvXbcNsEsoyBQOo9BWl\nENats8JvNoc8dM5Ea/WEWNMXsQ2ySWv7WtNXlA5DZO6Ixxit6UMY0od0EY9KX1E6kLnm4Dl50i4P\nuGpV8WXyyYoVttfS9LT9fyjSX7YseWOuxjuK0oHMVdPvxFo+2IViVq+Gs2ft/0ORvtb0FUWJhUr/\nVuojnlCkn6avvvbTV5QOZGAAjhx5Oc6IUOnD1av2fSlLbbgVafrqaz99RelAFiyAjRvh8OFX3q/S\nt39r1pRjmoJ2aLyjKEpsGiOea9fgxAnYutVbkbxSL/0Qoh3QeEdRlAQ0Sn9szE7G1tPjr0w+CVH6\nGu8oihKbRul3crQD4Upf4x1FUWKh0n8lIUo/abxjjEpfUTqWDRvsgKRz5+z/VfrhST9pvHP1Ksyb\nB729+ZUpCSp9RSmQ+ukYjOnMOXfq6e+38xGNj4cl/SQ1/TLV8kGlryiFE0k/mnyt7DNL5snKlVaK\np0+HI/2k0zCUaQoGUOkrSuFEc/CMjNjbIfRNz4t586z4DxwIR/pa01cUJRFRTb/T8/yI/n64fDk8\n6c+1NkIjwUlfRHpF5EkReUZERkVkd5NtbheRJ0TkaRF5VkTeUrv/DhG5Urv/aRH5dB4HoSghsXOn\n7Z//9NMqfbCy7+6G5ct9lyQe8+fbyeKuXo23fdninbYrchpjrovI/caYKyIyD/iBiHzDGPPjus0+\nDHzRGPNnIrIdeBy4s/bYMWPMfe6LrihhsmgRrF8Pjz8OH/yg79L4p7/fTsHQFVDuENX2+/rabxtc\nTR/AGHOldrMX+0PReGEzCyyp3V4GnK57rIMTS0VpzuCg7bo5MOC7JP7p7w8n2olI0le/TFMwQIya\nPoCIdAF7gdcAnzLGPNWwyR7gWyLy74E+YFfdYxtFZC9wGfgvxpj/l73YihI2g4Nw6JCNCjqdEKW/\ndCkcO2aXTmzHqVN23YCyEEv6xphZ4F4RWQI8JiIDxpixuk3+KfA/jDF/IiI/D/xPYAfwPLDBGHNB\nRO6re+5U42s89NBDL90eGhpiaGgo7TEpSum5//74mXDVueceO/FcSNx3H7z3vfG3/+Qn3bzu8PAw\nw8PDmfYhJm4TdPQEkY8AU8aYP667bz/wK8aY07X//y3wBmPMuYbnfhf4gDHm6Yb7TdJyKIqidDoi\ngjEmUYQep/fOKhFZWru9ABvdHGrY7O9q91NryO01xpyrPberdv8mYDPwXJICKoqiKO6IE++sAx6p\nybsL20vncRHZAzxljPka8J+APxeR/4ht1H137bl/H/ioiNwAbgK/ZYxJOCmpoiiK4orE8U4uhdB4\nR1EUJTG5xDuKoihKdVDpK4qidBAqfUVRlA5Cpa8oitJBqPQLIOtgirKjxxc2VT6+Kh9bWlT6BVD1\nD54eX9hU+fiqfGxpUekriqJ0ECp9RVGUDqI0g7N8l0FRFCVEkg7OKoX0FUVRlGLQeEdRFKWDUOkr\niqJ0EN6lLyIPiMghETkiIv/Zd3lcIyInRGRfbWH5H7d/RrkRkc+KyISIjNTdt1xEviUih0Xkm9FU\n3CEyx/HtFpFTIvJ07e8Bn2VMi4i8WkSeEJExERmtrXRXmfPX5Pj+Xe3+qpy/XhF5suaSURHZXbt/\no4j8qHb+/o+ItJw92WumX5uu+QjwD4AzwFPAu4wxjfP1B4uIPAe8zhhzwXdZXCAibwSmgL80xgzW\n7vsD4Lwx5g9rP9zLjTG/47OcaZnj+HYDk/ULB4WIiKwF1hpjnhWRRdglUN8O/AsqcP5aHN8/oQLn\nD0BE+owxV0RkHvAD4H3A+4EvGWP+WkT+O/CsMebP5tqH75r+64Gjxpi/M8bcAL6APUlVQvD/Pjuj\ntsZx4w/Y24FHarcfAX690EI5ZI7jA3seg8YYM26MebZ2ewo4CLyaipy/OY7vttrDwZ8/AGPMldrN\nXux6KAa4H3i0dv8jwD9qtQ/fMroNOFn3/1O8fJKqggG+KSJPici/8l2YnFhjjJkA+8UDSrQMtDP+\nrYg8KyJ/EWr8UY+IbATuAX4E9Fft/NUd35O1uypx/kSkS0SeAcaBbwN/C1ysrWMO1qHrW+3Dt/Sb\n/fpWrQ/pLxhj/h7wVuwH742+C6Qk5tPAa4wx92C/bEHHBLXo40vA+2o14kp955ocX2XOnzFm1hhz\nL/YK7fXA9mabtdqHb+mfAjbU/f/V2Gy/MtRqThhjXgD+L/ZEVY0JEemHl3LVs57L4xRjzAt1S7v9\nOfBzPsuThVoj35eAzxtjvly7uzLnr9nxVen8RRhjLgPfA34eWBatRU4Mh/qW/lPAZhG5Q0R6gHcB\nX/FcJmeISF+t1oGILAR+Gdjvt1ROEF55lfYV4D212+8Gvtz4hMB4xfHVRBjxIGGfw88BY8aYT9Td\nV6Xzd8vxVeX8iciqKJoSkQXALmAM+C7wztpmbc+f9xG5te5Tn8D+AH3WGPP7XgvkEBG5E1u7N9hG\nl/8V+vGJyP8GhoCVwASwG3gM+GvgduCnwDuNMRd9lTELcxzf/dh8eBY4AfxWlIGHhIj8IvB9YBT7\nmTTA7wI/Bv6KwM9fi+P7Z1Tj/N2Fbajtqv190RjzsZpnvgAsB54B/nmtY0zz/fiWvqIoilIcvuMd\nRVEUpUBU+oqiKB2ESl9RFKWDUOkriqJ0ECp9RVGUDkKlryiK0kGo9BVFUToIlb6iKEoH8f8B/SCE\nzyJ3otoAAAAASUVORK5CYII=\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x7feb183e54a8>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "if __name__ == '__main__':\n",
    "    main()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
    "    \"\"\"\n",
    "    \n",
    "if __name__ == '__main__':\n",
    "    main()\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
