{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from __future__ import division\n",
    "\n",
    "import gym\n",
    "import numpy as np\n",
    "import random\n",
    "import tensorflow as tf\n",
    "import tensorflow.contrib.layers as initializer\n",
    "import matplotlib.pyplot as plt\n",
    "import scipy.misc\n",
    "import os\n",
    "import sys\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from gridworld_v04 import gameEnv\n",
    "env = gameEnv(partial=False, size=15)\n",
    "#env.renderEnv()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Agent():\n",
    "    def __init__(self, hiddens = [256, 256, 256], num_frames = 4, state_size = 10,\n",
    "                 action_size = 6, env=None, lr = 1e-3, batch_size = 32, num_episodes = 10000, pre_train_steps = 10000,\n",
    "                 max_ep_length = 500, buffer_size = 50000, start_e = 1, final_e = 0.01, gamma = 0.99,\n",
    "                 tau=0.001, update_freq = 5, load_model = False , logs_dir = \"./logs_dir\"):\n",
    "\n",
    "        self.hiddens = hiddens # Size of the hidden layers\n",
    "        self.num_frames = num_frames # Number of consecutive state frames\n",
    "        self.state_size = state_size # Size of the state vector\n",
    "        self.action_size = action_size  # Number of actions\n",
    "        self.env = env # Save the environmet!\n",
    "        self.lr=lr # learning rate of the optimizer\n",
    "        self.batch_size=batch_size # Size of the experience sample batch\n",
    "        self.num_episodes = num_episodes # Number of game environmet episodes in which we train\n",
    "        self.pre_train_steps = pre_train_steps\n",
    "        self.max_ep_length = max_ep_length # Maximun length (number of actions) of a single train episode\n",
    "        self.buffer_size = buffer_size # Size of the experience replay buffer\n",
    "        self.start_e = start_e # Inital value of the exploration coefficient\n",
    "        self.final_e = final_e # Final value of the exploration coefficient\n",
    "        self.gamma = gamma # Discount factor on the target Q-value\n",
    "        self.tau = tau # Porcentage that determines how much are parameters of mainQN net modified by targetQN\n",
    "        self.update_freq = update_freq # Frecuency of updates of the double DQN\n",
    "        self.logs_dir = logs_dir # Path to store logs and checkpoints\n",
    "\n",
    "        tf.reset_default_graph()\n",
    "\n",
    "        # Instantiate the networks\n",
    "        self.mainQN = DQN(\"mainQN\", state_size, action_size, hiddens, num_frames)\n",
    "        self.targetQN = DQN(\"targetQN\", state_size, action_size, hiddens, num_frames)\n",
    "\n",
    "        #init = tf.global_variables_initializer()\n",
    "        trainables = tf.trainable_variables()\n",
    "\n",
    "        self.target_ops = self.set_target_graph_vars(trainables, tau)\n",
    "\n",
    "        # Create a experience replay buffer & score records\n",
    "        self.exp_buffer = ExperienceBuffer()\n",
    "        self.step_record = [] # for learing process\n",
    "        self.reward_record = [] # for learnig process\n",
    "        self.step_record_test = [] # for testing process\n",
    "        self.reward_record_test = [] # for testing process\n",
    "\n",
    "        self.model_saver = ModelSaver(self.logs_dir)\n",
    "        \n",
    "        self.file_manager = FileManager()\n",
    "        self.file_name_train = self.logs_dir + \"/train_logs\"\n",
    "        self.file_name_test = self.logs_dir + \"/test_logs\"\n",
    "\n",
    "    def learn(self):\n",
    "        print(\"Model Learning\")\n",
    "        init = tf.global_variables_initializer()\n",
    "        sess = tf.Session()\n",
    "        e = self.start_e\n",
    "        current_step = 0\n",
    "        \n",
    "        delimiter = \"\"\n",
    "        for _ in range(20):\n",
    "            delimiter += \"-\"\n",
    "        \n",
    "        #Code for writing to a file?\n",
    "        #logs_file = self.logs_dir + \"/train_logs.txt\"\n",
    "        #with open(logs_file) as fp:\n",
    "        # -> I made class FileManager below\n",
    "\n",
    "        with sess:\n",
    "\n",
    "            sess.run(init)\n",
    "           # if self.load_model == True:\n",
    "           #     print('Loading Model...')\n",
    "           #     model_saver.restore_model(sess)\n",
    "           #     #ckpt = model_saver_restore_model(sess)\n",
    "\n",
    "            # Set the target network to be equal to the primary network\n",
    "            self.update_target_graph(self.target_ops, sess)\n",
    "\n",
    "            # Start the pre train proces\n",
    "            for episode in range(self.num_episodes):\n",
    "\n",
    "                if episode % 100 == 0:\n",
    "                    print(\"\\n=====\" + \"Episode \" + str(episode) + \" starts =====\" )\n",
    "\n",
    "                    \n",
    "                self.file_manager.write(self.file_name_train, \"episode {0}\".format(episode+1))    \n",
    "                episode_exp = ExperienceBuffer()\n",
    "\n",
    "                #Reset environment and get first new observation\n",
    "                s = self.env.reset()\n",
    "\n",
    "                d = False # episode's \"done\" signal\n",
    "                episode_reward_sum = 0\n",
    "                episode_step = 0\n",
    "\n",
    "                #The Q-Network\n",
    "                while episode_step < self.max_ep_length: #If the agent take too long to win, end the trial.\n",
    "                    episode_step += 1\n",
    "\n",
    "                    #Choose an action by greedily (with e chance of random action) from the Q-network\n",
    "                    if np.random.rand(1) < e or current_step < self.pre_train_steps:\n",
    "                        a = np.random.randint(0,4)\n",
    "                    else:\n",
    "                        a = sess.run(self.mainQN.predict,feed_dict={self.mainQN.input:[s]})[0]\n",
    "\n",
    "                    s1,r,d = self.env.step(a)\n",
    "                    current_step += 1\n",
    "                    episode_exp.add(np.reshape(np.array([s,a,r,s1,d]),[1,5])) # Save the experience to our episode buffer.\n",
    "\n",
    "                    # Start train process\n",
    "                    if current_step > self.pre_train_steps:\n",
    "\n",
    "                        if e > self.final_e:\n",
    "                            stepDrop = 1/10000\n",
    "                            e -= stepDrop\n",
    "\n",
    "                        if current_step % self.update_freq == 0:\n",
    "\n",
    "                            train_batch = self.exp_buffer.sample(self.batch_size) #Get a random batch of experiences.\n",
    "\n",
    "                            #Perform the Double-DQN update to the target Q-values\n",
    "                            Q1 = sess.run(self.mainQN.predict,\n",
    "                                          feed_dict={self.mainQN.input:np.vstack(train_batch[:,3])})\n",
    "\n",
    "                            Q2 = sess.run(self.targetQN.Qout,\n",
    "                                          feed_dict={self.targetQN.input:np.vstack(train_batch[:,3])})\n",
    "\n",
    "                            end_multiplier = -(train_batch[:,4] - 1)\n",
    "                            doubleQ = Q2[range(self.batch_size),Q1]\n",
    "                            targetQ = train_batch[:,2] + (self.gamma*doubleQ*end_multiplier)\n",
    "\n",
    "                            # Update the network with our target values.\n",
    "                            _ = sess.run(self.mainQN.updateModel,\n",
    "                                         feed_dict={self.mainQN.input:np.vstack(train_batch[:,0]),\n",
    "                                         self.mainQN.targetQ:targetQ,\n",
    "                                         self.mainQN.actions:train_batch[:,1]})\n",
    "\n",
    "                            # Set the target network to be equal to the primary\n",
    "                            self.update_target_graph(self.target_ops, sess)\n",
    "\n",
    "                    episode_reward_sum += r\n",
    "                    s = s1\n",
    "\n",
    "                    if d == True:\n",
    "                        break\n",
    "\n",
    "                self.exp_buffer.add(episode_exp.buffer)\n",
    "                self.step_record.append(episode_step)\n",
    "                self.reward_record.append(episode_reward_sum)\n",
    "\n",
    "                self.file_manager.write(self.file_name_train, \"step_record: \"+\",\".join(str(step) for step in self.step_record))\n",
    "                self.file_manager.write(self.file_name_train, \"reward_record: \"+\",\".join(str(step) for step in self.reward_record))\n",
    "                \n",
    "                #Periodically save the model.\n",
    "                if episode % 1000 == 0:\n",
    "                    self.model_saver.save_model(sess, episode)\n",
    "                    print(\"Model saved\")\n",
    "\n",
    "                if len(self.reward_record) % 10 == 0:\n",
    "                    partial_reward = np.mean(self.reward_record[-10:])\n",
    "                    log = str(current_step) +\"\\t\"+ str(partial_reward) +\"\\t\"+ str(e) +\"\\n\"\n",
    "                    #f.write(log)\n",
    "                    print(\"%d %4.2f %3.2f\" % (current_step, partial_reward, e))\n",
    "\n",
    "                self.file_manager.write(self.file_name_train, delimiter)\n",
    "                \n",
    "            self.model_saver.save_model(sess, self.num_episodes)\n",
    "        final_log = \"Percent of sucessful episodes: \" + str(sum(self.reward_record)/self.num_episodes) + \"%\"\n",
    "        print(final_log)\n",
    "\n",
    "    def test(self):\n",
    "        current_step = 0\n",
    "        load_model = True\n",
    "        num_test_episodes = 5000\n",
    "\n",
    "        delimiter = \"\"\n",
    "        for _ in range(20):\n",
    "            delimiter += \"-\"\n",
    "        \n",
    "        with tf.Session() as sess:\n",
    "            if load_model == True:\n",
    "                print('Loading Model...')\n",
    "                self.model_saver.restore_model(sess)\n",
    "\n",
    "            for episode in range(num_test_episodes):\n",
    "                self.file_manager.write(self.file_name_test, \"episode {0}\".format(episode+1))    \n",
    "                \n",
    "                s = self.env.reset_for_testing()\n",
    "                d = False\n",
    "                episode_step = 0\n",
    "                episode_reword_sum = 0\n",
    "\n",
    "                #Q-Network\n",
    "                while episode_step < self.max_ep_length:\n",
    "                    episode_step += 1\n",
    "                    a = sess.run(self.mainQN.predict, feed_dict={self.mainQN.input:[s]})[0]\n",
    "                    s1, r, d = self.env.step_for_testing(a)\n",
    "\n",
    "                    current_step += 1\n",
    "                    episode_reword_sum += r\n",
    "                    s = s1\n",
    "\n",
    "                    if d == True:\n",
    "                        break\n",
    "\n",
    "                self.step_record_test.append(episode_step)\n",
    "                self.reward_record_test.append(episode_reword_sum)\n",
    "\n",
    "                self.file_manager.write(self.file_name_test, \"step_record_test: \"+\",\".join(str(step) for step in self.step_record_test))\n",
    "                self.file_manager.write(self.file_name_test, \"reward_record_tset: \"+\",\".join(str(step) for step in self.reward_record_test))\n",
    "                self.file_manager.write(self.file_name_test, delimiter)\n",
    "                \n",
    "                # print(episode_reword_sum)\n",
    "\n",
    "                if len(self.reward_record_test) % 10 == 0:\n",
    "                    partial_reward = np.mean(self.reward_record_test[-10:])\n",
    "                    log = str(current_step) +\"\\t\"+ str(partial_reward)\n",
    "                    #f.write(log)\n",
    "                    print(\"%d %4.2f\" % (current_step, partial_reward))\n",
    "\n",
    "        final_log = \"Percent of sucessful episodes: \" + str(sum(self.reward_record_test)/num_test_episodes) + \"%\"\n",
    "        print(final_log)\n",
    "\n",
    "    \"\"\" Auxiliary Methods \"\"\"\n",
    "    # Originally called updateTargetGraph\n",
    "    def set_target_graph_vars(self, tfVars, tau):\n",
    "        total_vars = len(tfVars)\n",
    "        op_holder = []\n",
    "\n",
    "        for idx,var in enumerate(tfVars[0:total_vars//2]): # Select the first half of the variables (mainQ net)\n",
    "            op_holder.append( tfVars[idx+total_vars//2].assign((var.value()*tau)+((1-tau)*tfVars[idx+total_vars//2].value())))\n",
    "\n",
    "        return op_holder\n",
    "    # Originally called updateTarget\n",
    "    def update_target_graph(self, op_holder, sess):\n",
    "        for op in op_holder:\n",
    "            sess.run(op)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class DQN():\n",
    "    def __init__(self, net_name, state_size, action_size, hiddens, num_frames):\n",
    "        self.net_name = net_name\n",
    "\n",
    "        with tf.variable_scope(self.net_name):\n",
    "\n",
    "            self.input = tf.placeholder(shape=[None, state_size], dtype=tf.float32)\n",
    "            #self.input_state = tf.reshape(self.state, [-1, num_frames * state_size])\n",
    "\n",
    "            # Weights of each layer\n",
    "            self.W = {\n",
    "                'W1': self.init_weight(\"W1\", [state_size, hiddens[0]]),\n",
    "                'W2': self.init_weight(\"W2\", [hiddens[0], hiddens[1]]),\n",
    "                'W3': self.init_weight(\"W3\", [hiddens[1], hiddens[2]]),\n",
    "                'AW': self.init_weight(\"AW\", [hiddens[2]//2, action_size]),\n",
    "                'VM': self.init_weight(\"VM\", [hiddens[2]//2, 1])\n",
    "            }\n",
    "\n",
    "            self.b = {\n",
    "                'b1': self.init_bias(\"b1\", hiddens[0]),\n",
    "                'b2': self.init_bias(\"b2\", hiddens[1]),\n",
    "                'b3': self.init_bias(\"b3\", hiddens[2])\n",
    "            }\n",
    "\n",
    "            # Layers\n",
    "            self.hidden1 = tf.nn.relu(tf.add(tf.matmul(self.input, self.W['W1']), self.b['b1']))\n",
    "            self.hidden2 = tf.nn.relu(tf.add(tf.matmul(self.hidden1, self.W['W2']), self.b['b2']))\n",
    "            self.hidden3 = tf.nn.relu(tf.add(tf.matmul(self.hidden2, self.W['W3']), self.b['b3']))\n",
    "\n",
    "            # Compute the Advantage, Value, and total Q value\n",
    "            self.A, self.V = tf.split(self.hidden3, 2, 1)\n",
    "            self.Advantage = tf.matmul(self.A, self.W['AW'])\n",
    "            self.Value = tf.matmul(self.V, self.W['VM'])\n",
    "            self.Qout = self.Value + tf.subtract(self.Advantage, tf.reduce_mean(self.Advantage, axis=1, keep_dims=True))\n",
    "\n",
    "            # Calcultate the action with highest Q value\n",
    "            self.predict = tf.argmax(self.Qout, 1)\n",
    "\n",
    "            # Compute the loss (sum of squared differences)\n",
    "            self.targetQ = tf.placeholder(shape=[None], dtype=tf.float32)\n",
    "            self.actions = tf.placeholder(shape=[None], dtype=tf.int32)\n",
    "            self.actions_one_hot = tf.one_hot(self.actions, action_size, dtype=tf.float32)\n",
    "\n",
    "            self.Q = tf.reduce_sum(tf.multiply(self.Qout, self.actions_one_hot), axis=1)\n",
    "            self.td_error = tf.square(self.targetQ - self.Q)\n",
    "            self.loss = tf.reduce_mean(self.td_error)\n",
    "\n",
    "            self.trainer = tf.train.AdamOptimizer(learning_rate=0.0001)\n",
    "            self.updateModel = self.trainer.minimize(self.loss)\n",
    "\n",
    "    def init_weight(self, name, shape):\n",
    "        return tf.get_variable(name=name, shape=shape, initializer=initializer.xavier_initializer(), dtype=tf.float32)\n",
    "\n",
    "    def init_bias(self, name, shape):\n",
    "        return tf.Variable(tf.random_normal([shape]))\n",
    "        #initializer = tf.constant(np.random.rand(shape))\n",
    "        #return tf.get_variable(name=name, initializer=initializer, dtype=tf.float32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class ExperienceBuffer():\n",
    "    def __init__(self, buffer_size = 50000):\n",
    "        self.buffer = []\n",
    "        self.buffer_size = buffer_size\n",
    "\n",
    "    def add(self, experience):\n",
    "\n",
    "        if len(self.buffer) + len(experience) >= self.buffer_size:\n",
    "            self.buffer[0:(len(experience) + len(self.buffer)) - self.buffer_size] = []\n",
    "\n",
    "        self.buffer.extend(experience)\n",
    "\n",
    "    def sample(self, size):\n",
    "        return np.reshape(np.array(random.sample(self.buffer, size)),[size,5])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class ModelSaver():\n",
    "    def __init__(self, path):\n",
    "        self.saver = tf.train.Saver()\n",
    "        self.ckptPath = path\n",
    "        if not os.path.exists(path):\n",
    "            os.makedirs(path)\n",
    "\n",
    "    def restore_model(self, sess):\n",
    "        ckpt = tf.train.get_checkpoint_state(self.ckptPath)\n",
    "        self.saver.restore(sess, ckpt.model_checkpoint_path)\n",
    "\n",
    "    def save_model(self, sess, num_episode):\n",
    "        self.saver.save(sess, self.ckptPath+'/model-'+str(num_episode)+'.cptk')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "class FileManager():\n",
    "    def __init__(self):\n",
    "        return \n",
    "        # If there is no file, create file with name firstly\\n\",\n",
    "        # If there is file with name, append content in the file \\n\",\n",
    "    def write(self, name=\"default_filename\", content=\"\"):\n",
    "        name = name+\".txt\"\n",
    "        with open(name, 'a') as f:\n",
    "            f.write(content)\n",
    "            f.write('\\n')\n",
    "        return\n",
    "\n",
    "    def read(self, name=\"default_filename\"):\n",
    "        name = name+\".txt\"\n",
    "        try:\n",
    "            with open(name, 'r') as f:\n",
    "                data = f.read()\n",
    "                return data\n",
    "        except FileNotFoundError as e:\n",
    "            print(e)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class DrawGraph(): # simply version\n",
    "    def __init__(self, reward_record):\n",
    "        self.reward_record = reward_record\n",
    "    def plot(self):\n",
    "        rMat = np.resize(np.array(self.reward_record), [len(self.reward_record)//100,100])           \n",
    "        rMean = np.average(rMat, 1)\n",
    "        plt.plot(rMean)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model Learning\n",
      "\n",
      "=====Episode 0 starts =====\n",
      "Model saved\n",
      "4374 -61.80 1.00\n",
      "8445 -64.70 1.00\n",
      "12450 -60.20 0.76\n",
      "17450 -38.20 0.26\n",
      "21455 -4.60 0.01\n",
      "26455 -6.10 0.01\n",
      "31455 -5.50 0.01\n",
      "35051 -1.70 0.01\n",
      "39666 -1.00 0.01\n",
      "43817 -0.80 0.01\n",
      "\n",
      "=====Episode 100 starts =====\n",
      "48817 -2.10 0.01\n",
      "50729 1.70 0.01\n",
      "53174 2.50 0.01\n",
      "55325 2.10 0.01\n",
      "57527 2.50 0.01\n",
      "59232 2.10 0.01\n",
      "60871 3.50 0.01\n",
      "62461 3.00 0.01\n",
      "64019 2.10 0.01\n",
      "65351 3.70 0.01\n",
      "\n",
      "=====Episode 200 starts =====\n",
      "66206 3.40 0.01\n",
      "67986 1.80 0.01\n",
      "69098 3.20 0.01\n",
      "70984 2.70 0.01\n",
      "71606 3.80 0.01\n",
      "72206 3.70 0.01\n",
      "73202 3.60 0.01\n",
      "73647 3.70 0.01\n",
      "74903 3.50 0.01\n",
      "75072 3.80 0.01\n",
      "\n",
      "=====Episode 300 starts =====\n",
      "76424 2.80 0.01\n",
      "76917 3.60 0.01\n",
      "77690 3.40 0.01\n",
      "78557 3.60 0.01\n",
      "79572 3.20 0.01\n",
      "80008 3.50 0.01\n",
      "80356 3.70 0.01\n",
      "81063 3.70 0.01\n",
      "81521 3.60 0.01\n",
      "82197 2.10 0.01\n",
      "\n",
      "=====Episode 400 starts =====\n",
      "82632 3.50 0.01\n",
      "83158 0.20 0.01\n",
      "83419 3.80 0.01\n",
      "83807 3.60 0.01\n",
      "84253 3.80 0.01\n",
      "84586 3.90 0.01\n",
      "84975 3.80 0.01\n",
      "85323 3.80 0.01\n",
      "85583 3.80 0.01\n",
      "85915 3.50 0.01\n",
      "\n",
      "=====Episode 500 starts =====\n",
      "86338 3.70 0.01\n",
      "86487 4.00 0.01\n",
      "87041 3.30 0.01\n",
      "87238 3.70 0.01\n",
      "87401 4.00 0.01\n",
      "87822 3.30 0.01\n",
      "88280 4.00 0.01\n",
      "88582 3.70 0.01\n",
      "88857 3.80 0.01\n",
      "89053 3.70 0.01\n",
      "\n",
      "=====Episode 600 starts =====\n",
      "89394 3.70 0.01\n",
      "89626 3.80 0.01\n",
      "89888 1.90 0.01\n",
      "90087 3.90 0.01\n",
      "90422 3.80 0.01\n",
      "90607 3.70 0.01\n",
      "91117 3.80 0.01\n",
      "91519 3.40 0.01\n",
      "91735 3.90 0.01\n",
      "91976 3.80 0.01\n",
      "\n",
      "=====Episode 700 starts =====\n",
      "92314 3.50 0.01\n",
      "92553 3.70 0.01\n",
      "92818 3.70 0.01\n",
      "93016 3.80 0.01\n",
      "93265 3.90 0.01\n",
      "93515 3.60 0.01\n",
      "93845 3.70 0.01\n",
      "94047 3.50 0.01\n",
      "94198 3.90 0.01\n",
      "94330 3.90 0.01\n",
      "\n",
      "=====Episode 800 starts =====\n",
      "94471 3.90 0.01\n",
      "94887 3.50 0.01\n",
      "95010 4.00 0.01\n",
      "95287 3.40 0.01\n",
      "95394 3.90 0.01\n",
      "95741 3.40 0.01\n",
      "95930 3.90 0.01\n",
      "96124 3.90 0.01\n",
      "96347 3.60 0.01\n",
      "96593 3.60 0.01\n",
      "\n",
      "=====Episode 900 starts =====\n",
      "96695 4.00 0.01\n",
      "96949 3.20 0.01\n",
      "97228 3.60 0.01\n",
      "97363 3.90 0.01\n",
      "97617 3.90 0.01\n",
      "97754 3.80 0.01\n",
      "97914 4.00 0.01\n",
      "98168 3.90 0.01\n",
      "98375 3.90 0.01\n",
      "98603 3.60 0.01\n",
      "\n",
      "=====Episode 1000 starts =====\n",
      "Model saved\n",
      "98851 3.80 0.01\n",
      "99074 3.70 0.01\n",
      "99318 3.80 0.01\n",
      "99418 3.80 0.01\n",
      "99847 3.40 0.01\n",
      "100011 3.70 0.01\n",
      "100260 3.50 0.01\n",
      "100331 3.90 0.01\n",
      "100543 3.80 0.01\n",
      "100770 3.60 0.01\n",
      "\n",
      "=====Episode 1100 starts =====\n",
      "101009 3.80 0.01\n",
      "101586 3.30 0.01\n",
      "101710 3.90 0.01\n",
      "101882 4.00 0.01\n",
      "102058 3.70 0.01\n",
      "102161 3.90 0.01\n",
      "102431 3.60 0.01\n",
      "102631 3.60 0.01\n",
      "102798 4.00 0.01\n",
      "102937 4.00 0.01\n",
      "\n",
      "=====Episode 1200 starts =====\n",
      "103071 3.70 0.01\n",
      "103144 4.00 0.01\n",
      "103285 3.80 0.01\n",
      "103403 3.90 0.01\n",
      "103557 3.80 0.01\n",
      "103748 4.00 0.01\n",
      "103896 3.90 0.01\n",
      "103993 4.00 0.01\n",
      "104191 3.90 0.01\n",
      "104391 3.90 0.01\n",
      "\n",
      "=====Episode 1300 starts =====\n",
      "104632 3.60 0.01\n",
      "104862 4.00 0.01\n",
      "104957 4.00 0.01\n",
      "105132 3.90 0.01\n",
      "105334 3.70 0.01\n",
      "105583 3.80 0.01\n",
      "105697 3.80 0.01\n",
      "105958 3.70 0.01\n",
      "106050 4.00 0.01\n",
      "106318 4.00 0.01\n",
      "\n",
      "=====Episode 1400 starts =====\n",
      "106503 3.80 0.01\n",
      "106639 3.80 0.01\n",
      "106771 4.00 0.01\n",
      "106900 3.90 0.01\n",
      "107138 3.90 0.01\n",
      "107271 4.00 0.01\n",
      "107410 3.90 0.01\n",
      "107530 3.90 0.01\n",
      "107598 4.00 0.01\n",
      "107726 4.00 0.01\n",
      "\n",
      "=====Episode 1500 starts =====\n",
      "107842 3.80 0.01\n",
      "108077 3.80 0.01\n",
      "108232 3.70 0.01\n",
      "108357 3.80 0.01\n",
      "108498 3.90 0.01\n",
      "108640 3.80 0.01\n",
      "108831 3.90 0.01\n",
      "109002 3.70 0.01\n",
      "109202 3.80 0.01\n",
      "109281 4.00 0.01\n",
      "\n",
      "=====Episode 1600 starts =====\n",
      "109350 4.00 0.01\n",
      "109473 4.00 0.01\n",
      "109724 3.60 0.01\n",
      "109815 3.80 0.01\n",
      "109976 3.90 0.01\n",
      "110102 3.90 0.01\n",
      "110258 4.00 0.01\n",
      "110398 4.00 0.01\n",
      "110604 3.70 0.01\n",
      "110782 4.00 0.01\n",
      "\n",
      "=====Episode 1700 starts =====\n",
      "110903 4.00 0.01\n",
      "111109 3.60 0.01\n",
      "111281 3.70 0.01\n",
      "111376 4.00 0.01\n",
      "111502 4.00 0.01\n",
      "111618 4.00 0.01\n",
      "111816 3.90 0.01\n",
      "111990 4.00 0.01\n",
      "112130 4.00 0.01\n",
      "112222 4.00 0.01\n",
      "\n",
      "=====Episode 1800 starts =====\n",
      "112304 4.00 0.01\n",
      "112468 3.90 0.01\n",
      "112658 3.90 0.01\n",
      "112773 4.00 0.01\n",
      "112940 3.80 0.01\n",
      "113036 4.00 0.01\n",
      "113214 3.90 0.01\n",
      "113423 3.80 0.01\n",
      "113559 3.90 0.01\n",
      "113646 3.90 0.01\n",
      "\n",
      "=====Episode 1900 starts =====\n",
      "113782 3.90 0.01\n",
      "113896 3.90 0.01\n",
      "114029 3.80 0.01\n",
      "114145 3.90 0.01\n",
      "114330 3.90 0.01\n",
      "114409 4.00 0.01\n",
      "114528 3.80 0.01\n",
      "114645 3.80 0.01\n",
      "114770 3.80 0.01\n",
      "114920 3.90 0.01\n",
      "\n",
      "=====Episode 2000 starts =====\n",
      "Model saved\n",
      "115027 4.00 0.01\n",
      "115176 3.80 0.01\n",
      "115322 3.60 0.01\n",
      "115418 4.00 0.01\n",
      "115534 3.90 0.01\n",
      "115635 3.90 0.01\n",
      "115821 3.80 0.01\n",
      "115899 4.00 0.01\n",
      "116018 3.90 0.01\n",
      "116224 3.80 0.01\n",
      "\n",
      "=====Episode 2100 starts =====\n",
      "116443 3.70 0.01\n",
      "116572 3.90 0.01\n",
      "116683 3.90 0.01\n",
      "116795 3.90 0.01\n",
      "116915 3.70 0.01\n",
      "117065 3.90 0.01\n",
      "117163 3.90 0.01\n",
      "117327 3.80 0.01\n",
      "117423 3.90 0.01\n",
      "117593 3.70 0.01\n",
      "\n",
      "=====Episode 2200 starts =====\n",
      "117692 4.00 0.01\n",
      "117787 3.90 0.01\n",
      "117893 4.00 0.01\n",
      "118007 3.90 0.01\n",
      "118137 3.90 0.01\n",
      "118263 3.80 0.01\n",
      "118428 4.00 0.01\n",
      "118567 3.80 0.01\n",
      "118657 4.00 0.01\n",
      "118829 3.90 0.01\n",
      "\n",
      "=====Episode 2300 starts =====\n",
      "118955 3.70 0.01\n",
      "119062 3.90 0.01\n",
      "119334 3.20 0.01\n",
      "119576 4.00 0.01\n",
      "119669 4.00 0.01\n",
      "119850 3.90 0.01\n",
      "119937 3.80 0.01\n",
      "120183 3.70 0.01\n",
      "120294 3.90 0.01\n",
      "120428 4.00 0.01\n",
      "\n",
      "=====Episode 2400 starts =====\n",
      "120534 3.90 0.01\n",
      "120654 3.70 0.01\n",
      "120776 3.90 0.01\n",
      "120895 3.90 0.01\n",
      "120963 4.00 0.01\n",
      "121052 4.00 0.01\n",
      "121153 4.00 0.01\n",
      "121290 3.90 0.01\n",
      "121417 3.90 0.01\n",
      "121503 3.80 0.01\n",
      "\n",
      "=====Episode 2500 starts =====\n",
      "121587 4.00 0.01\n",
      "121703 3.90 0.01\n",
      "121851 3.80 0.01\n",
      "121991 3.70 0.01\n",
      "122146 3.70 0.01\n",
      "122284 3.90 0.01\n",
      "122438 3.70 0.01\n",
      "122580 3.90 0.01\n",
      "122752 3.80 0.01\n",
      "122845 4.00 0.01\n",
      "\n",
      "=====Episode 2600 starts =====\n",
      "122949 3.90 0.01\n",
      "123031 4.00 0.01\n",
      "123265 3.80 0.01\n",
      "123353 4.00 0.01\n",
      "123412 4.00 0.01\n",
      "123519 3.90 0.01\n",
      "123642 3.90 0.01\n",
      "123746 3.90 0.01\n",
      "123880 3.80 0.01\n",
      "124044 3.80 0.01\n",
      "\n",
      "=====Episode 2700 starts =====\n",
      "124189 3.90 0.01\n",
      "124298 3.90 0.01\n",
      "124381 4.00 0.01\n",
      "124475 3.90 0.01\n",
      "124601 3.80 0.01\n",
      "124715 3.90 0.01\n",
      "124817 3.90 0.01\n",
      "124941 3.90 0.01\n",
      "125078 3.90 0.01\n",
      "125245 3.70 0.01\n",
      "\n",
      "=====Episode 2800 starts =====\n",
      "125367 3.80 0.01\n",
      "125468 3.80 0.01\n",
      "125566 4.00 0.01\n",
      "125640 4.00 0.01\n",
      "125750 3.90 0.01\n",
      "125892 3.60 0.01\n",
      "126035 3.70 0.01\n",
      "126205 3.80 0.01\n",
      "126355 3.90 0.01\n",
      "126427 4.00 0.01\n",
      "\n",
      "=====Episode 2900 starts =====\n",
      "126515 4.00 0.01\n",
      "126651 3.90 0.01\n",
      "126764 3.70 0.01\n",
      "126843 4.00 0.01\n",
      "126934 3.90 0.01\n",
      "127046 3.90 0.01\n",
      "127158 3.80 0.01\n",
      "127295 3.80 0.01\n",
      "127363 4.00 0.01\n",
      "127505 3.80 0.01\n",
      "\n",
      "=====Episode 3000 starts =====\n",
      "Model saved\n",
      "127593 4.00 0.01\n",
      "127725 3.90 0.01\n",
      "127873 3.90 0.01\n",
      "128002 3.70 0.01\n",
      "128098 4.00 0.01\n",
      "128218 3.80 0.01\n",
      "128307 4.00 0.01\n",
      "128398 4.00 0.01\n",
      "128547 3.80 0.01\n",
      "128721 3.40 0.01\n",
      "\n",
      "=====Episode 3100 starts =====\n",
      "128871 3.80 0.01\n",
      "129011 3.80 0.01\n",
      "129164 3.90 0.01\n",
      "129248 4.00 0.01\n",
      "129376 3.70 0.01\n",
      "129552 3.90 0.01\n",
      "129689 3.70 0.01\n",
      "129828 3.80 0.01\n",
      "129911 4.00 0.01\n",
      "129996 4.00 0.01\n",
      "\n",
      "=====Episode 3200 starts =====\n",
      "130132 3.90 0.01\n",
      "130253 3.70 0.01\n",
      "130372 4.00 0.01\n",
      "130528 3.80 0.01\n",
      "130616 4.00 0.01\n",
      "130767 3.80 0.01\n",
      "130971 3.90 0.01\n",
      "131075 3.90 0.01\n",
      "131188 3.60 0.01\n",
      "131420 3.40 0.01\n",
      "\n",
      "=====Episode 3300 starts =====\n",
      "131518 3.90 0.01\n",
      "131637 4.00 0.01\n",
      "131713 3.90 0.01\n",
      "131867 4.00 0.01\n",
      "131930 4.00 0.01\n",
      "132100 3.70 0.01\n",
      "132358 2.40 0.01\n",
      "132440 3.90 0.01\n",
      "132585 4.00 0.01\n",
      "132707 3.90 0.01\n",
      "\n",
      "=====Episode 3400 starts =====\n",
      "132799 4.00 0.01\n",
      "132925 3.80 0.01\n",
      "133007 4.00 0.01\n",
      "133134 3.80 0.01\n",
      "133275 3.90 0.01\n",
      "133413 3.90 0.01\n",
      "133537 3.80 0.01\n",
      "133800 3.30 0.01\n",
      "133909 3.90 0.01\n",
      "134039 3.70 0.01\n",
      "\n",
      "=====Episode 3500 starts =====\n",
      "134142 3.90 0.01\n",
      "134364 3.90 0.01\n",
      "134500 4.00 0.01\n",
      "134619 3.80 0.01\n",
      "134828 3.70 0.01\n",
      "134962 3.90 0.01\n",
      "135095 4.00 0.01\n",
      "135238 4.00 0.01\n",
      "135461 4.00 0.01\n",
      "135687 3.90 0.01\n",
      "\n",
      "=====Episode 3600 starts =====\n",
      "135820 3.90 0.01\n",
      "135986 4.00 0.01\n",
      "136187 3.20 0.01\n",
      "136597 2.50 0.01\n",
      "136694 4.00 0.01\n",
      "136895 3.00 0.01\n",
      "137197 2.90 0.01\n",
      "137399 3.70 0.01\n",
      "137600 3.50 0.01\n",
      "137801 3.90 0.01\n",
      "\n",
      "=====Episode 3700 starts =====\n",
      "137940 3.70 0.01\n",
      "138097 3.90 0.01\n",
      "138180 3.90 0.01\n",
      "138430 3.90 0.01\n",
      "138560 2.50 0.01\n",
      "138787 3.90 0.01\n",
      "139094 3.90 0.01\n",
      "139198 3.90 0.01\n",
      "139305 3.90 0.01\n",
      "139586 3.60 0.01\n",
      "\n",
      "=====Episode 3800 starts =====\n",
      "139741 3.90 0.01\n",
      "139945 2.50 0.01\n",
      "140247 3.90 0.01\n",
      "140472 4.00 0.01\n",
      "140681 3.90 0.01\n",
      "140871 3.70 0.01\n",
      "141040 4.00 0.01\n",
      "141127 4.00 0.01\n",
      "141256 3.90 0.01\n",
      "141371 3.80 0.01\n",
      "\n",
      "=====Episode 3900 starts =====\n",
      "141581 3.90 0.01\n",
      "141802 3.90 0.01\n",
      "141946 3.80 0.01\n",
      "142060 3.80 0.01\n",
      "142283 3.80 0.01\n",
      "142485 3.70 0.01\n",
      "142637 3.70 0.01\n",
      "142772 4.00 0.01\n",
      "142867 4.00 0.01\n",
      "142995 2.80 0.01\n",
      "\n",
      "=====Episode 4000 starts =====\n",
      "Model saved\n",
      "143101 4.00 0.01\n",
      "143410 3.80 0.01\n",
      "143506 4.00 0.01\n",
      "143632 3.80 0.01\n",
      "143782 3.80 0.01\n",
      "143870 4.00 0.01\n",
      "144022 3.60 0.01\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "144202 4.00 0.01\n",
      "144402 4.00 0.01\n",
      "144550 4.00 0.01\n",
      "\n",
      "=====Episode 4100 starts =====\n",
      "144732 3.70 0.01\n",
      "144798 4.00 0.01\n",
      "144961 3.80 0.01\n",
      "145074 4.00 0.01\n",
      "145193 3.90 0.01\n",
      "145330 3.80 0.01\n",
      "145451 3.90 0.01\n",
      "145637 3.80 0.01\n",
      "145901 3.60 0.01\n",
      "146169 3.90 0.01\n",
      "\n",
      "=====Episode 4200 starts =====\n",
      "146279 3.90 0.01\n",
      "146372 4.00 0.01\n",
      "146495 3.90 0.01\n",
      "146711 3.80 0.01\n",
      "146876 3.90 0.01\n",
      "146994 3.90 0.01\n",
      "147053 4.00 0.01\n",
      "147166 3.70 0.01\n",
      "147336 3.70 0.01\n",
      "147529 3.70 0.01\n",
      "\n",
      "=====Episode 4300 starts =====\n",
      "147700 3.90 0.01\n",
      "147851 3.90 0.01\n",
      "147939 4.00 0.01\n",
      "148096 3.90 0.01\n",
      "148338 3.80 0.01\n",
      "148519 3.90 0.01\n",
      "148608 4.00 0.01\n",
      "148697 4.00 0.01\n",
      "148807 3.90 0.01\n",
      "148952 3.90 0.01\n",
      "\n",
      "=====Episode 4400 starts =====\n",
      "149044 4.00 0.01\n",
      "149238 3.90 0.01\n",
      "149435 3.80 0.01\n",
      "149579 3.90 0.01\n",
      "149737 3.90 0.01\n",
      "149875 3.90 0.01\n",
      "149982 4.00 0.01\n",
      "150206 3.60 0.01\n",
      "150332 3.80 0.01\n",
      "150418 4.00 0.01\n",
      "\n",
      "=====Episode 4500 starts =====\n",
      "150584 3.80 0.01\n",
      "150686 3.80 0.01\n",
      "150909 4.00 0.01\n",
      "151032 3.70 0.01\n",
      "151153 3.70 0.01\n",
      "151293 3.80 0.01\n",
      "151460 3.90 0.01\n",
      "151585 4.00 0.01\n",
      "151663 4.00 0.01\n",
      "151798 3.70 0.01\n",
      "\n",
      "=====Episode 4600 starts =====\n",
      "151931 3.90 0.01\n",
      "152116 4.00 0.01\n",
      "152249 3.80 0.01\n",
      "152437 3.90 0.01\n",
      "152561 3.90 0.01\n",
      "152674 4.00 0.01\n",
      "152775 4.00 0.01\n",
      "152875 4.00 0.01\n",
      "152974 3.80 0.01\n",
      "153124 3.80 0.01\n",
      "\n",
      "=====Episode 4700 starts =====\n",
      "153276 3.90 0.01\n",
      "153382 3.90 0.01\n",
      "153509 3.80 0.01\n",
      "153613 4.00 0.01\n",
      "153723 4.00 0.01\n",
      "153912 3.80 0.01\n",
      "154016 4.00 0.01\n",
      "154126 3.90 0.01\n",
      "154357 3.60 0.01\n",
      "154658 4.00 0.01\n",
      "\n",
      "=====Episode 4800 starts =====\n",
      "154859 3.60 0.01\n",
      "154984 3.80 0.01\n",
      "155109 3.90 0.01\n",
      "155353 3.90 0.01\n",
      "155551 3.50 0.01\n",
      "155679 3.70 0.01\n",
      "155787 4.00 0.01\n",
      "156036 3.90 0.01\n",
      "156200 4.00 0.01\n",
      "156301 3.90 0.01\n",
      "\n",
      "=====Episode 4900 starts =====\n",
      "156515 4.00 0.01\n",
      "156618 3.90 0.01\n",
      "156836 3.70 0.01\n",
      "156970 4.00 0.01\n",
      "157071 4.00 0.01\n",
      "157164 4.00 0.01\n",
      "157302 3.90 0.01\n",
      "157364 4.00 0.01\n",
      "157610 3.80 0.01\n",
      "157751 3.90 0.01\n",
      "Percent of sucessful episodes: 3.1966%\n",
      "Loading Model...\n",
      "INFO:tensorflow:Restoring parameters from ./logs_dir/model-5000.cptk\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[2017-06-13 05:41:14,005] Restoring parameters from ./logs_dir/model-5000.cptk\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1290447 1.60\n",
      "Percent of sucessful episodes: 1.4908%\n"
     ]
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAXwAAAEACAYAAACwB81wAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x7f3ce289f048>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "def main():\n",
    "    env = gameEnv(partial = False, size=15)\n",
    "        \n",
    "    # The parameters of Agent class are as follows:\n",
    "    # hiddens, num_frames, state_size, action_size, env, lr, batch_size, num_episodes, pre_train_steps,\n",
    "    # max_ep_length, buffer_size, start_e, final_e, gamma, tau, update_freq, load_model, logs_dir\n",
    "    agent = Agent([256, 256, 256], 1, len(env.mapEnv()), 4, env, 1e-3, 32, 10000, 10000, 500, 50000, 1, 0.01, 0.99, 0.001, 5, False, \"./logs_dir\")\n",
    "    agent.learn()\n",
    "    agent.test()\n",
    "    \n",
    "    train_graph = DrawGraph(agent.reward_record)\n",
    "    test_graph = DrawGraph(agent.reward_record_test)\n",
    "    \n",
    "    train_graph.plot()\n",
    "    test_graph.plot()\n",
    "    \n",
    "\n",
    "if __name__ == '__main__':\n",
    "    main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.4.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
